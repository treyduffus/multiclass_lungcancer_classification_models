{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group 33, Florida Atlantic University\n",
    "# Implementation of Somepalli, Gowthami, et al. “SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training.” ArXiv:2106.01342 [Cs, Stat], 2 June 2021, arxiv.org/abs/2106.01342.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn, einsum\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "import argparse\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = [\n",
    "    'SAINT.py', \n",
    "    '--pretrain',\n",
    "    '--task', 'multiclass',\n",
    "    '--target_label', 'general'\n",
    "]\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument('--dataset_name', type=str, choices=['miRNA', 'methelyene'], default = 'miRNA')\n",
    "parser.add_argument('--target_label', type=str, choices=['general', 'stage', 'subtype'])\n",
    "parser.add_argument('--vision_dset', action = 'store_true')\n",
    "parser.add_argument('--task', required=True, type=str,choices = ['binary','multiclass','regression'], default='multiclass')\n",
    "parser.add_argument('--cont_embeddings', default='MLP', type=str,choices = ['MLP','Noemb','pos_singleMLP'])\n",
    "parser.add_argument('--embedding_size', default=32, type=int)\n",
    "parser.add_argument('--transformer_depth', default=6, type=int)\n",
    "parser.add_argument('--attention_heads', default=8, type=int)\n",
    "parser.add_argument('--attention_dropout', default=0.1, type=float)\n",
    "parser.add_argument('--ff_dropout', default=0.1, type=float)\n",
    "parser.add_argument('--attentiontype', default='colrow', type=str,choices = ['col','colrow','row','justmlp','attn','attnmlp'])\n",
    "\n",
    "parser.add_argument('--optimizer', default='AdamW', type=str,choices = ['AdamW','Adam','SGD'])\n",
    "parser.add_argument('--scheduler', default='cosine', type=str,choices = ['cosine','linear'])\n",
    "\n",
    "parser.add_argument('--lr', default=0.0001, type=float)\n",
    "parser.add_argument('--epochs', default=100, type=int)\n",
    "parser.add_argument('--batchsize', default=256, type=int)\n",
    "parser.add_argument('--savemodelroot', default='./bestmodels', type=str)\n",
    "parser.add_argument('--run_name', default='testrun', type=str)\n",
    "parser.add_argument('--set_seed', default= 1 , type=int)\n",
    "parser.add_argument('--dset_seed', default= 1 , type=int)\n",
    "parser.add_argument('--active_log', action = 'store_true')\n",
    "\n",
    "parser.add_argument('--pretrain', action = 'store_true')\n",
    "parser.add_argument('--pretrain_epochs', default=50, type=int)\n",
    "parser.add_argument('--pt_tasks', default=['contrastive','denoising'], type=str,nargs='*',choices = ['contrastive','contrastive_sim','denoising'])\n",
    "parser.add_argument('--pt_aug', default=[], type=str,nargs='*',choices = ['mixup','cutmix'])\n",
    "parser.add_argument('--pt_aug_lam', default=0.1, type=float)\n",
    "parser.add_argument('--mixup_lam', default=0.3, type=float)\n",
    "\n",
    "parser.add_argument('--train_noise_type', default=None , type=str,choices = ['missing','cutmix'])\n",
    "parser.add_argument('--train_noise_level', default=0, type=float)\n",
    "\n",
    "parser.add_argument('--ssl_samples', default= None, type=int)\n",
    "parser.add_argument('--pt_projhead_style', default='diff', type=str,choices = ['diff','same','nohead'])\n",
    "parser.add_argument('--nce_temp', default=0.7, type=float)\n",
    "\n",
    "parser.add_argument('--lam0', default=0.5, type=float)\n",
    "parser.add_argument('--lam1', default=10, type=float)\n",
    "parser.add_argument('--lam2', default=1, type=float)\n",
    "parser.add_argument('--lam3', default=10, type=float)\n",
    "parser.add_argument('--final_mlp_style', default='sep', type=str,choices = ['common','sep'])\n",
    "\n",
    "opt = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_lapsed_time(text, lapsed):\n",
    "    hours, rem = divmod(lapsed, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    print(text+\": {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "\n",
    "def concat_data(X,y):\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    return pd.concat([pd.DataFrame(X['data']), pd.DataFrame(y['data'][:,0].tolist(),columns=['target'])], axis=1)\n",
    "\n",
    "\n",
    "def data_split(X,y,nan_mask,indices):\n",
    "    x_d = {\n",
    "        'data': X.values[indices],\n",
    "        'mask': nan_mask.values[indices]\n",
    "    }\n",
    "    \n",
    "    if x_d['data'].shape != x_d['mask'].shape:\n",
    "        raise'Shape of data not same as that of nan mask!'\n",
    "        \n",
    "    y_d = {\n",
    "        'data': y[indices].reshape(-1, 1)\n",
    "    } \n",
    "    return x_d, y_d\n",
    "\n",
    "\n",
    "def data_prep(task, datasplit=[.65, .15, .2]): # 80/20 -> 65/15/20\n",
    "    \n",
    "    dataset = pd.read_csv(f'../processed_data/{opt.dataset_name}_stage_subtype.csv')\n",
    "    \n",
    "    # TODO: Review target attribute value\n",
    "    # This doesnt work because the dataset is alr a dataframe\n",
    "\n",
    "    dataset = dataset.loc[:, dataset.std() > 0] # removing features with 0 variance\n",
    "\n",
    "    pos_neg_labels, stage_labels, subtype_labels = dataset.iloc[:, -2], dataset.iloc[:, -2], dataset.iloc[:, -1]\n",
    "    X = dataset.iloc[:, : -2]\n",
    "\n",
    "    scaler = RobustScaler() # Robust Scaler is more resistant to outliers\n",
    "    X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "    y = []\n",
    "\n",
    "    match opt.target_label:\n",
    "        case 'general':\n",
    "            y = pos_neg_labels\n",
    "        case 'stage':\n",
    "            y = stage_labels\n",
    "        case 'subtype':\n",
    "            y = subtype_labels\n",
    "\n",
    "    categorical_columns = X.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
    "    cont_columns = list(set(X.columns) - set(categorical_columns))\n",
    "\n",
    "    cat_idxs = [X.columns.get_loc(col) for col in categorical_columns]\n",
    "    con_idxs = [X.columns.get_loc(col) for col in cont_columns]\n",
    "    for col in categorical_columns:\n",
    "        X[col] = X[col].astype(\"object\")\n",
    "\n",
    "    X[\"Set\"] = np.random.choice([\"train\", \"valid\", \"test\"], p = datasplit, size=(X.shape[0],))\n",
    "\n",
    "    train_indices = X[X.Set==\"train\"].index\n",
    "    valid_indices = X[X.Set==\"valid\"].index\n",
    "    test_indices = X[X.Set==\"test\"].index\n",
    "\n",
    "    X = X.drop(columns=['Set'])\n",
    "    temp = X.fillna(\"MissingValue\")\n",
    "    nan_mask = temp.ne(\"MissingValue\").astype(int)\n",
    "    \n",
    "    cat_dims = []\n",
    "    for col in categorical_columns:\n",
    "    #     X[col] = X[col].cat.add_categories(\"MissingValue\")\n",
    "        X[col] = X[col].fillna(\"MissingValue\")\n",
    "        l_enc = LabelEncoder() \n",
    "        X[col] = l_enc.fit_transform(X[col].values)\n",
    "        cat_dims.append(len(l_enc.classes_))\n",
    "    for col in cont_columns:\n",
    "    #     X[col].fillna(\"MissingValue\",inplace=True)\n",
    "        X.fillna(X.loc[train_indices, col].mean(), inplace=True)\n",
    "    y = y.values\n",
    "    if task != 'regression':\n",
    "        l_enc = LabelEncoder() \n",
    "        y = l_enc.fit_transform(y)\n",
    "    X_train, y_train = data_split(X,y,nan_mask,train_indices)\n",
    "    X_valid, y_valid = data_split(X,y,nan_mask,valid_indices)\n",
    "    X_test, y_test = data_split(X,y,nan_mask,test_indices)\n",
    "\n",
    "    train_mean, train_std = np.array(X_train['data'][:,con_idxs],dtype=np.float32).mean(0), np.array(X_train['data'][:,con_idxs],dtype=np.float32).std(0)\n",
    "    train_std = np.where(train_std < 1e-6, 1e-6, train_std)\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    return cat_dims, cat_idxs, con_idxs, X_train, y_train, X_valid, y_valid, X_test, y_test, train_mean, train_std\n",
    "\n",
    "\n",
    "class DataSetCatCon(Dataset):\n",
    "    def __init__(self, X, Y, cat_cols,task='clf',continuous_mean_std=None):\n",
    "        \n",
    "        cat_cols = list(cat_cols)\n",
    "        X_mask =  X['mask'].copy()\n",
    "        X = X['data'].copy()\n",
    "        con_cols = list(set(np.arange(X.shape[1])) - set(cat_cols))\n",
    "        self.X1 = X[:,cat_cols].copy().astype(np.int64) #categorical columns\n",
    "        self.X2 = X[:,con_cols].copy().astype(np.float32) #numerical columns\n",
    "        self.X1_mask = X_mask[:,cat_cols].copy().astype(np.int64) #categorical columns\n",
    "        self.X2_mask = X_mask[:,con_cols].copy().astype(np.int64) #numerical columns\n",
    "        if task == 'clf':\n",
    "            self.y = Y['data']#.astype(np.float32)\n",
    "        else:\n",
    "            self.y = Y['data'].astype(np.float32)\n",
    "        self.cls = np.zeros_like(self.y,dtype=int)\n",
    "        self.cls_mask = np.ones_like(self.y,dtype=int)\n",
    "        if continuous_mean_std is not None:\n",
    "            mean, std = continuous_mean_std\n",
    "            self.X2 = (self.X2 - mean) / std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # X1 has categorical data, X2 has continuous\n",
    "        return np.concatenate((self.cls[idx], self.X1[idx])), self.X2[idx],self.y[idx], np.concatenate((self.cls_mask[idx], self.X1_mask[idx])), self.X2_mask[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_data_mask(x_categ, x_cont, cat_mask, con_mask,model,vision_dset=False):\n",
    "    device = x_cont.device\n",
    "    x_categ = x_categ + model.categories_offset.type_as(x_categ)\n",
    "    x_categ_enc = model.embeds(x_categ)\n",
    "    n1,n2 = x_cont.shape\n",
    "    _, n3 = x_categ.shape\n",
    "    if model.cont_embeddings == 'MLP':\n",
    "        x_cont_enc = torch.empty(n1,n2, model.dim)\n",
    "        for i in range(model.num_continuous):\n",
    "            x_cont_enc[:,i,:] = model.simple_MLP[i](x_cont[:,i])\n",
    "    else:\n",
    "        raise Exception('This case should not work!')    \n",
    "\n",
    "\n",
    "    x_cont_enc = x_cont_enc.to(device)\n",
    "    cat_mask_temp = cat_mask + model.cat_mask_offset.type_as(cat_mask)\n",
    "    con_mask_temp = con_mask + model.con_mask_offset.type_as(con_mask)\n",
    "\n",
    "\n",
    "    cat_mask_temp = model.mask_embeds_cat(cat_mask_temp)\n",
    "    con_mask_temp = model.mask_embeds_cont(con_mask_temp)\n",
    "    x_categ_enc[cat_mask == 0] = cat_mask_temp[cat_mask == 0]\n",
    "    x_cont_enc[con_mask == 0] = con_mask_temp[con_mask == 0]\n",
    "\n",
    "    if vision_dset:\n",
    "        \n",
    "        pos = np.tile(np.arange(x_categ.shape[-1]),(x_categ.shape[0],1))\n",
    "        pos =  torch.from_numpy(pos).to(device)\n",
    "        pos_enc =model.pos_encodings(pos)\n",
    "        x_categ_enc+=pos_enc\n",
    "\n",
    "    return x_categ, x_categ_enc, x_cont_enc\n",
    "\n",
    "\n",
    "def mixup_data(x1, x2 , lam=1.0, y= None, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets'''\n",
    "\n",
    "    batch_size = x1.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "\n",
    "    mixed_x1 = lam * x1 + (1 - lam) * x1[index, :]\n",
    "    mixed_x2 = lam * x2 + (1 - lam) * x2[index, :]\n",
    "    if y is not None:\n",
    "        y_a, y_b = y, y[index]\n",
    "        return mixed_x1, mixed_x2, y_a, y_b\n",
    "    \n",
    "    return mixed_x1, mixed_x2\n",
    "\n",
    "\n",
    "def add_noise(x_categ,x_cont, noise_params = {'noise_type' : ['cutmix'],'lambda' : 0.1}):\n",
    "    lam = noise_params['lambda']\n",
    "    device = x_categ.device\n",
    "    batch_size = x_categ.size()[0]\n",
    "\n",
    "    if 'cutmix' in noise_params['noise_type']:\n",
    "        index = torch.randperm(batch_size)\n",
    "        cat_corr = torch.from_numpy(np.random.choice(2,(x_categ.shape),p=[lam,1-lam])).to(device)\n",
    "        con_corr = torch.from_numpy(np.random.choice(2,(x_cont.shape),p=[lam,1-lam])).to(device)\n",
    "        x1, x2 =  x_categ[index,:], x_cont[index,:]\n",
    "        x_categ_corr, x_cont_corr = x_categ.clone().detach() ,x_cont.clone().detach()\n",
    "        x_categ_corr[cat_corr==0] = x1[cat_corr==0]\n",
    "        x_cont_corr[con_corr==0] = x2[con_corr==0]\n",
    "        return x_categ_corr, x_cont_corr\n",
    "    elif noise_params['noise_type'] == 'missing':\n",
    "        x_categ_mask = np.random.choice(2,(x_categ.shape),p=[lam,1-lam])\n",
    "        x_cont_mask = np.random.choice(2,(x_cont.shape),p=[lam,1-lam])\n",
    "        x_categ_mask = torch.from_numpy(x_categ_mask).to(device)\n",
    "        x_cont_mask = torch.from_numpy(x_cont_mask).to(device)\n",
    "        return torch.mul(x_categ,x_categ_mask), torch.mul(x_cont,x_cont_mask)\n",
    "        \n",
    "    else:\n",
    "        print(\"yet to write this\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_default_mask(x):\n",
    "    mask = np.ones_like(x)\n",
    "    mask[:,-1] = 0\n",
    "    return mask\n",
    "\n",
    "def tag_gen(tag,y):\n",
    "    return np.repeat(tag,len(y['data']))\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)  \n",
    "\n",
    "def get_scheduler(args, optimizer):\n",
    "    if args.scheduler == 'cosine':\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs)\n",
    "    elif args.scheduler == 'linear':\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                      milestones=[args.epochs // 2.667, args.epochs // 1.6, args.epochs // 1.142], gamma=0.1)\n",
    "    return scheduler\n",
    "\n",
    "def imputations_acc_justy(model,dloader,device):\n",
    "    model.eval()\n",
    "    m = nn.Softmax(dim=1)\n",
    "    y_test = torch.empty(0).to(device)\n",
    "    y_pred = torch.empty(0).to(device)\n",
    "    prob = torch.empty(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dloader, 0):\n",
    "            x_categ, x_cont, cat_mask, con_mask = data[0].to(device), data[1].to(device),data[2].to(device),data[3].to(device)\n",
    "            _ , x_categ_enc, x_cont_enc = embed_data_mask(x_categ, x_cont, cat_mask, con_mask,model)\n",
    "            reps = model.transformer(x_categ_enc, x_cont_enc)\n",
    "            y_reps = reps[:,model.num_categories-1,:]\n",
    "            y_outs = model.mlpfory(y_reps)\n",
    "            # import ipdb; ipdb.set_trace()   \n",
    "            y_test = torch.cat([y_test,x_categ[:,-1].float()],dim=0)\n",
    "            y_pred = torch.cat([y_pred,torch.argmax(m(y_outs), dim=1).float()],dim=0)\n",
    "            prob = torch.cat([prob,m(y_outs)[:,-1].float()],dim=0)\n",
    "     \n",
    "    correct_results_sum = (y_pred == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]*100\n",
    "    auc = roc_auc_score(y_score=prob.cpu(), y_true=y_test.cpu())\n",
    "    return acc, auc\n",
    "\n",
    "\n",
    "def multiclass_acc_justy(model,dloader,device):\n",
    "    model.eval()\n",
    "    vision_dset = True\n",
    "    m = nn.Softmax(dim=1)\n",
    "    y_test = torch.empty(0).to(device)\n",
    "    y_pred = torch.empty(0).to(device)\n",
    "    prob = torch.empty(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dloader, 0):\n",
    "            x_categ, x_cont, cat_mask, con_mask = data[0].to(device), data[1].to(device),data[2].to(device),data[3].to(device)\n",
    "            _ , x_categ_enc, x_cont_enc = embed_data_mask(x_categ, x_cont, cat_mask, con_mask,model,vision_dset)\n",
    "            reps = model.transformer(x_categ_enc, x_cont_enc)\n",
    "            y_reps = reps[:,model.num_categories-1,:]\n",
    "            y_outs = model.mlpfory(y_reps)\n",
    "            # import ipdb; ipdb.set_trace()   \n",
    "            y_test = torch.cat([y_test,x_categ[:,-1].float()],dim=0)\n",
    "            y_pred = torch.cat([y_pred,torch.argmax(m(y_outs), dim=1).float()],dim=0)\n",
    "     \n",
    "    correct_results_sum = (y_pred == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]*100\n",
    "    return acc, 0\n",
    "\n",
    "\n",
    "def classification_scores(model, dloader, device, task,vision_dset):\n",
    "    model.eval()\n",
    "    m = nn.Softmax(dim=1)\n",
    "    y_test = torch.empty(0).to(device)\n",
    "    y_pred = torch.empty(0).to(device)\n",
    "    prob = torch.empty(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dloader, 0):\n",
    "            x_categ, x_cont, y_gts, cat_mask, con_mask = data[0].to(device), data[1].to(device),data[2].to(device),data[3].to(device),data[4].to(device)\n",
    "            _ , x_categ_enc, x_cont_enc = embed_data_mask(x_categ, x_cont, cat_mask, con_mask,model,vision_dset)           \n",
    "            reps = model.transformer(x_categ_enc, x_cont_enc)\n",
    "            y_reps = reps[:,0,:]\n",
    "            y_outs = model.mlpfory(y_reps)\n",
    "            # import ipdb; ipdb.set_trace()   \n",
    "            y_test = torch.cat([y_test,y_gts.squeeze().float()],dim=0)\n",
    "            y_pred = torch.cat([y_pred,torch.argmax(y_outs, dim=1).float()],dim=0)\n",
    "            if task == 'binary':\n",
    "                prob = torch.cat([prob,m(y_outs)[:,-1].float()],dim=0)\n",
    "     \n",
    "    correct_results_sum = (y_pred == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]*100\n",
    "    auc = 0\n",
    "    if task == 'binary':\n",
    "        auc = roc_auc_score(y_score=prob.cpu(), y_true=y_test.cpu())\n",
    "    return acc.cpu().numpy(), auc\n",
    "\n",
    "def mean_sq_error(model, dloader, device, vision_dset):\n",
    "    model.eval()\n",
    "    y_test = torch.empty(0).to(device)\n",
    "    y_pred = torch.empty(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dloader, 0):\n",
    "            x_categ, x_cont, y_gts, cat_mask, con_mask = data[0].to(device), data[1].to(device),data[2].to(device),data[3].to(device),data[4].to(device)\n",
    "            _ , x_categ_enc, x_cont_enc = embed_data_mask(x_categ, x_cont, cat_mask, con_mask,model,vision_dset)           \n",
    "            reps = model.transformer(x_categ_enc, x_cont_enc)\n",
    "            y_reps = reps[:,0,:]\n",
    "            y_outs = model.mlpfory(y_reps)\n",
    "            y_test = torch.cat([y_test,y_gts.squeeze().float()],dim=0)\n",
    "            y_pred = torch.cat([y_pred,y_outs],dim=0)\n",
    "        # import ipdb; ipdb.set_trace() \n",
    "        rmse = mean_squared_error(y_test.cpu(), y_pred.cpu(), squared=False)\n",
    "        return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def ff_encodings(x,B):\n",
    "    x_proj = (2. * np.pi * x.unsqueeze(-1)) @ B.t()\n",
    "    return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
    "\n",
    "# classes\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "# attention\n",
    "\n",
    "class GEGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gates = x.chunk(2, dim = -1)\n",
    "        return x * F.gelu(gates)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult = 4, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult * 2),\n",
    "            GEGLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * mult, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        heads = 8,\n",
    "        dim_head = 16,\n",
    "        dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.heads\n",
    "        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n",
    "        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class RowColTransformer(nn.Module):\n",
    "    def __init__(self, num_tokens, dim, nfeats, depth, heads, dim_head, attn_dropout, ff_dropout,style='col'):\n",
    "        super().__init__()\n",
    "        self.embeds = nn.Embedding(num_tokens, dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.mask_embed =  nn.Embedding(nfeats, dim)\n",
    "        self.style = style\n",
    "        for _ in range(depth):\n",
    "            if self.style == 'colrow':\n",
    "                self.layers.append(nn.ModuleList([\n",
    "                    PreNorm(dim, Residual(Attention(dim, heads = heads, dim_head = dim_head, dropout = attn_dropout))),\n",
    "                    PreNorm(dim, Residual(FeedForward(dim, dropout = ff_dropout))),\n",
    "                    PreNorm(dim*nfeats, Residual(Attention(dim*nfeats, heads = heads, dim_head = 64, dropout = attn_dropout))),\n",
    "                    PreNorm(dim*nfeats, Residual(FeedForward(dim*nfeats, dropout = ff_dropout))),\n",
    "                ]))\n",
    "            else:\n",
    "                self.layers.append(nn.ModuleList([\n",
    "                    PreNorm(dim*nfeats, Residual(Attention(dim*nfeats, heads = heads, dim_head = 64, dropout = attn_dropout))),\n",
    "                    PreNorm(dim*nfeats, Residual(FeedForward(dim*nfeats, dropout = ff_dropout))),\n",
    "                ]))\n",
    "\n",
    "    def forward(self, x, x_cont=None, mask = None):\n",
    "        if x_cont is not None:\n",
    "            x = torch.cat((x,x_cont),dim=1)\n",
    "        _, n, _ = x.shape\n",
    "        if self.style == 'colrow':\n",
    "            for attn1, ff1, attn2, ff2 in self.layers: \n",
    "                x = attn1(x)\n",
    "                x = ff1(x)\n",
    "                x = rearrange(x, 'b n d -> 1 b (n d)')\n",
    "                x = attn2(x)\n",
    "                x = ff2(x)\n",
    "                x = rearrange(x, '1 b (n d) -> b n d', n = n)\n",
    "        else:\n",
    "             for attn1, ff1 in self.layers:\n",
    "                x = rearrange(x, 'b n d -> 1 b (n d)')\n",
    "                x = attn1(x)\n",
    "                x = ff1(x)\n",
    "                x = rearrange(x, '1 b (n d) -> b n d', n = n)\n",
    "        return x\n",
    "\n",
    "\n",
    "# transformer\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_tokens, dim, depth, heads, dim_head, attn_dropout, ff_dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Residual(Attention(dim, heads = heads, dim_head = dim_head, dropout = attn_dropout))),\n",
    "                PreNorm(dim, Residual(FeedForward(dim, dropout = ff_dropout))),\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x, x_cont=None):\n",
    "        if x_cont is not None:\n",
    "            x = torch.cat((x,x_cont),dim=1)\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x)\n",
    "            x = ff(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "#mlp\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dims, act = None):\n",
    "        super().__init__()\n",
    "        dims_pairs = list(zip(dims[:-1], dims[1:]))\n",
    "        layers = []\n",
    "        for ind, (dim_in, dim_out) in enumerate(dims_pairs):\n",
    "            is_last = ind >= (len(dims) - 1)\n",
    "            linear = nn.Linear(dim_in, dim_out)\n",
    "            layers.append(linear)\n",
    "\n",
    "            if is_last:\n",
    "                continue\n",
    "            if act is not None:\n",
    "                layers.append(act)\n",
    "\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "class simple_MLP(nn.Module):\n",
    "    def __init__(self,dims):\n",
    "        super(simple_MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(dims[0], dims[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dims[1], dims[2])\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if len(x.shape)==1:\n",
    "            x = x.view(x.size(0), -1)\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "# main class\n",
    "\n",
    "class TabAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        categories,\n",
    "        num_continuous,\n",
    "        dim,\n",
    "        depth,\n",
    "        heads,\n",
    "        dim_head = 16,\n",
    "        dim_out = 1,\n",
    "        mlp_hidden_mults = (4, 2),\n",
    "        mlp_act = None,\n",
    "        num_special_tokens = 1,\n",
    "        continuous_mean_std = None,\n",
    "        attn_dropout = 0.,\n",
    "        ff_dropout = 0.,\n",
    "        lastmlp_dropout = 0.,\n",
    "        cont_embeddings = 'MLP',\n",
    "        scalingfactor = 10,\n",
    "        attentiontype = 'col'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert all(map(lambda n: n > 0, categories)), 'number of each category must be positive'\n",
    "\n",
    "        # categories related calculations\n",
    "        self.num_categories = len(categories)\n",
    "        self.num_unique_categories = sum(categories)\n",
    "\n",
    "        # create category embeddings table\n",
    "\n",
    "        self.num_special_tokens = num_special_tokens\n",
    "        self.total_tokens = self.num_unique_categories + num_special_tokens\n",
    "\n",
    "        # for automatically offsetting unique category ids to the correct position in the categories embedding table\n",
    "        categories_offset = F.pad(torch.tensor(list(categories)), (1, 0), value = num_special_tokens)\n",
    "        categories_offset = categories_offset.cumsum(dim = -1)[:-1]\n",
    "        \n",
    "        self.register_buffer('categories_offset', categories_offset)\n",
    "\n",
    "\n",
    "        self.norm = nn.LayerNorm(num_continuous)\n",
    "        self.num_continuous = num_continuous\n",
    "        self.dim = dim\n",
    "        self.cont_embeddings = cont_embeddings\n",
    "        self.attentiontype = attentiontype\n",
    "\n",
    "        if self.cont_embeddings == 'MLP':\n",
    "            self.simple_MLP = nn.ModuleList([simple_MLP([1,100,self.dim]) for _ in range(self.num_continuous)])\n",
    "            input_size = (dim * self.num_categories)  + (dim * num_continuous)\n",
    "            nfeats = self.num_categories + num_continuous\n",
    "        else:\n",
    "            print('Continous features are not passed through attention')\n",
    "            input_size = (dim * self.num_categories) + num_continuous\n",
    "            nfeats = self.num_categories \n",
    "\n",
    "        # transformer\n",
    "        if attentiontype == 'col':\n",
    "            self.transformer = Transformer(\n",
    "                num_tokens = self.total_tokens,\n",
    "                dim = dim,\n",
    "                depth = depth,\n",
    "                heads = heads,\n",
    "                dim_head = dim_head,\n",
    "                attn_dropout = attn_dropout,\n",
    "                ff_dropout = ff_dropout\n",
    "            )\n",
    "        elif attentiontype in ['row','colrow'] :\n",
    "            self.transformer = RowColTransformer(\n",
    "                num_tokens = self.total_tokens,\n",
    "                dim = dim,\n",
    "                nfeats= nfeats,\n",
    "                depth = depth,\n",
    "                heads = heads,\n",
    "                dim_head = dim_head,\n",
    "                attn_dropout = attn_dropout,\n",
    "                ff_dropout = ff_dropout,\n",
    "                style = attentiontype\n",
    "            )\n",
    "\n",
    "        l = input_size // 8\n",
    "        hidden_dimensions = list(map(lambda t: l * t, mlp_hidden_mults))\n",
    "        all_dimensions = [input_size, *hidden_dimensions, dim_out]\n",
    "        \n",
    "        self.mlp = MLP(all_dimensions, act = mlp_act)\n",
    "        self.embeds = nn.Embedding(self.total_tokens, self.dim) #.to(device)\n",
    "\n",
    "        cat_mask_offset = F.pad(torch.Tensor(self.num_categories).fill_(2).type(torch.int8), (1, 0), value = 0) \n",
    "        cat_mask_offset = cat_mask_offset.cumsum(dim = -1)[:-1]\n",
    "\n",
    "        con_mask_offset = F.pad(torch.Tensor(self.num_continuous).fill_(2).type(torch.int8), (1, 0), value = 0) \n",
    "        con_mask_offset = con_mask_offset.cumsum(dim = -1)[:-1]\n",
    "\n",
    "        self.register_buffer('cat_mask_offset', cat_mask_offset)\n",
    "        self.register_buffer('con_mask_offset', con_mask_offset)\n",
    "\n",
    "        self.mask_embeds_cat = nn.Embedding(self.num_categories*2, self.dim)\n",
    "        self.mask_embeds_cont = nn.Embedding(self.num_continuous*2, self.dim)\n",
    "\n",
    "    def forward(self, x_categ, x_cont,x_categ_enc,x_cont_enc):\n",
    "        device = x_categ.device\n",
    "        if self.attentiontype == 'justmlp':\n",
    "            if x_categ.shape[-1] > 0:\n",
    "                flat_categ = x_categ.flatten(1).to(device)\n",
    "                x = torch.cat((flat_categ, x_cont.flatten(1).to(device)), dim = -1)\n",
    "            else:\n",
    "                x = x_cont.clone()\n",
    "        else:\n",
    "            if self.cont_embeddings == 'MLP':\n",
    "                x = self.transformer(x_categ_enc,x_cont_enc.to(device))\n",
    "            else:\n",
    "                if x_categ.shape[-1] <= 0:\n",
    "                    x = x_cont.clone()\n",
    "                else: \n",
    "                    flat_categ = self.transformer(x_categ_enc).flatten(1)\n",
    "                    x = torch.cat((flat_categ, x_cont), dim = -1)                    \n",
    "        flat_x = x.flatten(1)\n",
    "        return self.mlp(flat_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sep_MLP(nn.Module):\n",
    "    def __init__(self,dim,len_feats,categories):\n",
    "        super(sep_MLP, self).__init__()\n",
    "        self.len_feats = len_feats\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for i in range(len_feats):\n",
    "            self.layers.append(simple_MLP([dim,5*dim, categories[i]]))\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = list([])\n",
    "        for i in range(self.len_feats):\n",
    "            x_i = x[:,i,:]\n",
    "            pred = self.layers[i](x_i)\n",
    "            y_pred.append(pred)\n",
    "        return y_pred\n",
    "\n",
    "class SAINT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        categories,\n",
    "        num_continuous,\n",
    "        dim,\n",
    "        depth,\n",
    "        heads,\n",
    "        dim_head = 16,\n",
    "        dim_out = 1,\n",
    "        mlp_hidden_mults = (4, 2),\n",
    "        mlp_act = None,\n",
    "        num_special_tokens = 0,\n",
    "        attn_dropout = 0.,\n",
    "        ff_dropout = 0.,\n",
    "        cont_embeddings = 'MLP',\n",
    "        scalingfactor = 10,\n",
    "        attentiontype = 'col',\n",
    "        final_mlp_style = 'common',\n",
    "        y_dim = 2\n",
    "        ):\n",
    "        super().__init__()\n",
    "        assert all(map(lambda n: n > 0, categories)), 'number of each category must be positive'\n",
    "\n",
    "        # categories related calculations\n",
    "\n",
    "        self.num_categories = len(categories)\n",
    "        self.num_unique_categories = sum(categories)\n",
    "\n",
    "        # create category embeddings table\n",
    "\n",
    "        self.num_special_tokens = num_special_tokens\n",
    "        self.total_tokens = self.num_unique_categories + num_special_tokens\n",
    "\n",
    "        # for automatically offsetting unique category ids to the correct position in the categories embedding table\n",
    "\n",
    "        categories_offset = F.pad(torch.tensor(list(categories)), (1, 0), value = num_special_tokens)\n",
    "        categories_offset = categories_offset.cumsum(dim = -1)[:-1]\n",
    "        \n",
    "        self.register_buffer('categories_offset', categories_offset)\n",
    "\n",
    "\n",
    "        self.norm = nn.LayerNorm(num_continuous)\n",
    "        self.num_continuous = num_continuous\n",
    "        self.dim = dim\n",
    "        self.cont_embeddings = cont_embeddings\n",
    "        self.attentiontype = attentiontype\n",
    "        self.final_mlp_style = final_mlp_style\n",
    "\n",
    "        if self.cont_embeddings == 'MLP':\n",
    "            self.simple_MLP = nn.ModuleList([simple_MLP([1,100,self.dim]) for _ in range(self.num_continuous)])\n",
    "            input_size = (dim * self.num_categories)  + (dim * num_continuous)\n",
    "            nfeats = self.num_categories + num_continuous\n",
    "        elif self.cont_embeddings == 'pos_singleMLP':\n",
    "            self.simple_MLP = nn.ModuleList([simple_MLP([1,100,self.dim]) for _ in range(1)])\n",
    "            input_size = (dim * self.num_categories)  + (dim * num_continuous)\n",
    "            nfeats = self.num_categories + num_continuous\n",
    "        else:\n",
    "            print('Continous features are not passed through attention')\n",
    "            input_size = (dim * self.num_categories) + num_continuous\n",
    "            nfeats = self.num_categories \n",
    "\n",
    "        # transformer\n",
    "        if attentiontype == 'col':\n",
    "            self.transformer = Transformer(\n",
    "                num_tokens = self.total_tokens,\n",
    "                dim = dim,\n",
    "                depth = depth,\n",
    "                heads = heads,\n",
    "                dim_head = dim_head,\n",
    "                attn_dropout = attn_dropout,\n",
    "                ff_dropout = ff_dropout\n",
    "            )\n",
    "        elif attentiontype in ['row','colrow'] :\n",
    "            self.transformer = RowColTransformer(\n",
    "                num_tokens = self.total_tokens,\n",
    "                dim = dim,\n",
    "                nfeats= nfeats,\n",
    "                depth = depth,\n",
    "                heads = heads,\n",
    "                dim_head = dim_head,\n",
    "                attn_dropout = attn_dropout,\n",
    "                ff_dropout = ff_dropout,\n",
    "                style = attentiontype\n",
    "            )\n",
    "\n",
    "        l = input_size // 8\n",
    "        hidden_dimensions = list(map(lambda t: l * t, mlp_hidden_mults))\n",
    "        all_dimensions = [input_size, *hidden_dimensions, dim_out]\n",
    "        \n",
    "        self.mlp = MLP(all_dimensions, act = mlp_act)\n",
    "        self.embeds = nn.Embedding(self.total_tokens, self.dim) #.to(device)\n",
    "\n",
    "        cat_mask_offset = F.pad(torch.Tensor(self.num_categories).fill_(2).type(torch.int8), (1, 0), value = 0) \n",
    "        cat_mask_offset = cat_mask_offset.cumsum(dim = -1)[:-1]\n",
    "\n",
    "        con_mask_offset = F.pad(torch.Tensor(self.num_continuous).fill_(2).type(torch.int8), (1, 0), value = 0) \n",
    "        con_mask_offset = con_mask_offset.cumsum(dim = -1)[:-1]\n",
    "\n",
    "        self.register_buffer('cat_mask_offset', cat_mask_offset)\n",
    "        self.register_buffer('con_mask_offset', con_mask_offset)\n",
    "\n",
    "        self.mask_embeds_cat = nn.Embedding(self.num_categories*2, self.dim)\n",
    "        self.mask_embeds_cont = nn.Embedding(self.num_continuous*2, self.dim)\n",
    "        self.single_mask = nn.Embedding(2, self.dim)\n",
    "        self.pos_encodings = nn.Embedding(self.num_categories+ self.num_continuous, self.dim)\n",
    "        \n",
    "        if self.final_mlp_style == 'common':\n",
    "            self.mlp1 = simple_MLP([dim,(self.total_tokens)*2, self.total_tokens])\n",
    "            self.mlp2 = simple_MLP([dim ,(self.num_continuous), 1])\n",
    "\n",
    "        else:\n",
    "            self.mlp1 = sep_MLP(dim,self.num_categories,categories)\n",
    "            self.mlp2 = sep_MLP(dim,self.num_continuous,np.ones(self.num_continuous).astype(int))\n",
    "\n",
    "\n",
    "        self.mlpfory = simple_MLP([dim ,1000, y_dim])\n",
    "        self.pt_mlp = simple_MLP([dim*(self.num_continuous+self.num_categories) ,6*dim*(self.num_continuous+self.num_categories)//5, dim*(self.num_continuous+self.num_categories)//2])\n",
    "        self.pt_mlp2 = simple_MLP([dim*(self.num_continuous+self.num_categories) ,6*dim*(self.num_continuous+self.num_categories)//5, dim*(self.num_continuous+self.num_categories)//2])\n",
    "\n",
    "        \n",
    "    def forward(self, x_categ, x_cont):\n",
    "        \n",
    "        x = self.transformer(x_categ, x_cont)\n",
    "        cat_outs = self.mlp1(x[:,:self.num_categories,:])\n",
    "        con_outs = self.mlp2(x[:,self.num_categories:,:])\n",
    "        return cat_outs, con_outs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SAINT_pretrain(model,cat_idxs,X_train,y_train,continuous_mean_std,opt,device):\n",
    "    train_ds = DataSetCatCon(X_train, y_train, cat_idxs,opt.dtask, continuous_mean_std)\n",
    "    trainloader = DataLoader(train_ds, batch_size=opt.batchsize, shuffle=True,num_workers=0)\n",
    "    vision_dset = opt.vision_dset\n",
    "    optimizer = optim.AdamW(model.parameters(),lr=0.0001)\n",
    "    pt_aug_dict = {\n",
    "        'noise_type' : opt.pt_aug,\n",
    "        'lambda' : opt.pt_aug_lam\n",
    "    }\n",
    "    criterion1 = nn.CrossEntropyLoss()\n",
    "    criterion2 = nn.MSELoss()\n",
    "    print(\"Pretraining begins!\")\n",
    "    for epoch in range(opt.pretrain_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            optimizer.zero_grad()\n",
    "            x_categ, x_cont, _ ,cat_mask, con_mask = data[0].to(device), data[1].to(device),data[2].to(device),data[3].to(device),data[4].to(device)\n",
    "            \n",
    "            # embed_data_mask function is used to embed both categorical and continuous data.\n",
    "            if 'cutmix' in opt.pt_aug:\n",
    "                x_categ_corr, x_cont_corr = add_noise(x_categ,x_cont, noise_params = pt_aug_dict)\n",
    "                _ , x_categ_enc_2, x_cont_enc_2 = embed_data_mask(x_categ_corr, x_cont_corr, cat_mask, con_mask,model,vision_dset)\n",
    "            else:\n",
    "                _ , x_categ_enc_2, x_cont_enc_2 = embed_data_mask(x_categ, x_cont, cat_mask, con_mask,model,vision_dset)\n",
    "            _ , x_categ_enc, x_cont_enc = embed_data_mask(x_categ, x_cont, cat_mask, con_mask,model,vision_dset)\n",
    "            \n",
    "            if 'mixup' in opt.pt_aug:\n",
    "                x_categ_enc_2, x_cont_enc_2 = mixup_data(x_categ_enc_2, x_cont_enc_2 , lam=opt.mixup_lam)\n",
    "            loss = 0\n",
    "            if 'contrastive' in opt.pt_tasks:\n",
    "                aug_features_1  = model.transformer(x_categ_enc, x_cont_enc)\n",
    "                aug_features_2 = model.transformer(x_categ_enc_2, x_cont_enc_2)\n",
    "                aug_features_1 = (aug_features_1 / aug_features_1.norm(dim=-1, keepdim=True)).flatten(1,2)\n",
    "                aug_features_2 = (aug_features_2 / aug_features_2.norm(dim=-1, keepdim=True)).flatten(1,2)\n",
    "                if opt.pt_projhead_style == 'diff':\n",
    "                    aug_features_1 = model.pt_mlp(aug_features_1)\n",
    "                    aug_features_2 = model.pt_mlp2(aug_features_2)\n",
    "                elif opt.pt_projhead_style == 'same':\n",
    "                    aug_features_1 = model.pt_mlp(aug_features_1)\n",
    "                    aug_features_2 = model.pt_mlp(aug_features_2)\n",
    "                else:\n",
    "                    print('Not using projection head')\n",
    "                logits_per_aug1 = aug_features_1 @ aug_features_2.t()/opt.nce_temp\n",
    "                logits_per_aug2 =  aug_features_2 @ aug_features_1.t()/opt.nce_temp\n",
    "                targets = torch.arange(logits_per_aug1.size(0)).to(logits_per_aug1.device)\n",
    "                loss_1 = criterion1(logits_per_aug1, targets)\n",
    "                loss_2 = criterion1(logits_per_aug2, targets)\n",
    "                loss   = opt.lam0*(loss_1 + loss_2)/2\n",
    "            elif 'contrastive_sim' in opt.pt_tasks:\n",
    "                aug_features_1  = model.transformer(x_categ_enc, x_cont_enc)\n",
    "                aug_features_2 = model.transformer(x_categ_enc_2, x_cont_enc_2)\n",
    "                aug_features_1 = (aug_features_1 / aug_features_1.norm(dim=-1, keepdim=True)).flatten(1,2)\n",
    "                aug_features_2 = (aug_features_2 / aug_features_2.norm(dim=-1, keepdim=True)).flatten(1,2)\n",
    "                aug_features_1 = model.pt_mlp(aug_features_1)\n",
    "                aug_features_2 = model.pt_mlp2(aug_features_2)\n",
    "                c1 = aug_features_1 @ aug_features_2.t()\n",
    "                loss+= opt.lam1*torch.diagonal(-1*c1).add_(1).pow_(2).sum()\n",
    "            if 'denoising' in opt.pt_tasks:\n",
    "                cat_outs, con_outs = model(x_categ_enc_2, x_cont_enc_2)\n",
    "                # if con_outs.shape(-1) != 0:\n",
    "                # import ipdb; ipdb.set_trace()\n",
    "                if len(con_outs) > 0:\n",
    "                    con_outs =  torch.cat(con_outs,dim=1)\n",
    "                    l2 = criterion2(con_outs, x_cont)\n",
    "                else:\n",
    "                    l2 = 0\n",
    "                l1 = 0\n",
    "                # import ipdb; ipdb.set_trace()\n",
    "                n_cat = x_categ.shape[-1]\n",
    "                for j in range(1,n_cat):\n",
    "                    l1+= criterion1(cat_outs[j],x_categ[:,j])\n",
    "                loss += opt.lam2*l1 + opt.lam3*l2    \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch: {epoch}, Running Loss: {running_loss}')\n",
    "\n",
    "    print('END OF PRETRAINING!')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cpu.\n",
      "Downloading and processing the dataset, it might take some time.\n",
      "1636 64\n",
      "Namespace(dataset_name='miRNA', target_label='general', vision_dset=False, task='multiclass', cont_embeddings='MLP', embedding_size=8, transformer_depth=1, attention_heads=4, attention_dropout=0.8, ff_dropout=0.8, attentiontype='colrow', optimizer='AdamW', scheduler='cosine', lr=0.0001, epochs=100, batchsize=64, savemodelroot='./bestmodels', run_name='testrun', set_seed=1, dset_seed=1, active_log=False, pretrain=True, pretrain_epochs=50, pt_tasks=['contrastive', 'denoising'], pt_aug=[], pt_aug_lam=0.1, mixup_lam=0.3, train_noise_type=None, train_noise_level=0, ssl_samples=None, pt_projhead_style='diff', nce_temp=0.7, lam0=0.5, lam1=10, lam2=1, lam3=10, final_mlp_style='sep', dtask='clf')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 51\u001b[0m\n\u001b[0;32m     45\u001b[0m     y_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][:,\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m     47\u001b[0m cat_dims \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m]),np\u001b[38;5;241m.\u001b[39marray(cat_dims))\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m) \u001b[38;5;66;03m#Appending 1 for CLS token, this is later used to generate embeddings.\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m model \u001b[38;5;241m=\u001b[39m SAINT(\n\u001b[0;32m     52\u001b[0m categories \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(cat_dims), \n\u001b[0;32m     53\u001b[0m num_continuous \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(con_idxs),                \n\u001b[0;32m     54\u001b[0m dim \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39membedding_size,                           \n\u001b[0;32m     55\u001b[0m dim_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,                       \n\u001b[0;32m     56\u001b[0m depth \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mtransformer_depth,                       \n\u001b[0;32m     57\u001b[0m heads \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mattention_heads,                         \n\u001b[0;32m     58\u001b[0m attn_dropout \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mattention_dropout,             \n\u001b[0;32m     59\u001b[0m ff_dropout \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mff_dropout,                  \n\u001b[0;32m     60\u001b[0m mlp_hidden_mults \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m2\u001b[39m),       \n\u001b[0;32m     61\u001b[0m cont_embeddings \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mcont_embeddings,\n\u001b[0;32m     62\u001b[0m attentiontype \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mattentiontype,\n\u001b[0;32m     63\u001b[0m final_mlp_style \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mfinal_mlp_style,\n\u001b[0;32m     64\u001b[0m y_dim \u001b[38;5;241m=\u001b[39m y_dim\n\u001b[0;32m     65\u001b[0m )\n\u001b[0;32m     66\u001b[0m vision_dset \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mvision_dset\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_dim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m opt\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# opt.task = 'binary'\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 93\u001b[0m, in \u001b[0;36mSAINT.__init__\u001b[1;34m(self, categories, num_continuous, dim, depth, heads, dim_head, dim_out, mlp_hidden_mults, mlp_act, num_special_tokens, attn_dropout, ff_dropout, cont_embeddings, scalingfactor, attentiontype, final_mlp_style, y_dim)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer \u001b[38;5;241m=\u001b[39m Transformer(\n\u001b[0;32m     84\u001b[0m         num_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_tokens,\n\u001b[0;32m     85\u001b[0m         dim \u001b[38;5;241m=\u001b[39m dim,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m         ff_dropout \u001b[38;5;241m=\u001b[39m ff_dropout\n\u001b[0;32m     91\u001b[0m     )\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m attentiontype \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrow\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolrow\u001b[39m\u001b[38;5;124m'\u001b[39m] :\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer \u001b[38;5;241m=\u001b[39m RowColTransformer(\n\u001b[0;32m     94\u001b[0m         num_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_tokens,\n\u001b[0;32m     95\u001b[0m         dim \u001b[38;5;241m=\u001b[39m dim,\n\u001b[0;32m     96\u001b[0m         nfeats\u001b[38;5;241m=\u001b[39m nfeats,\n\u001b[0;32m     97\u001b[0m         depth \u001b[38;5;241m=\u001b[39m depth,\n\u001b[0;32m     98\u001b[0m         heads \u001b[38;5;241m=\u001b[39m heads,\n\u001b[0;32m     99\u001b[0m         dim_head \u001b[38;5;241m=\u001b[39m dim_head,\n\u001b[0;32m    100\u001b[0m         attn_dropout \u001b[38;5;241m=\u001b[39m attn_dropout,\n\u001b[0;32m    101\u001b[0m         ff_dropout \u001b[38;5;241m=\u001b[39m ff_dropout,\n\u001b[0;32m    102\u001b[0m         style \u001b[38;5;241m=\u001b[39m attentiontype\n\u001b[0;32m    103\u001b[0m     )\n\u001b[0;32m    105\u001b[0m l \u001b[38;5;241m=\u001b[39m input_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m    106\u001b[0m hidden_dimensions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m t: l \u001b[38;5;241m*\u001b[39m t, mlp_hidden_mults))\n",
      "Cell \u001b[1;32mIn[7], line 92\u001b[0m, in \u001b[0;36mRowColTransformer.__init__\u001b[1;34m(self, num_tokens, dim, nfeats, depth, heads, dim_head, attn_dropout, ff_dropout, style)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(depth):\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstyle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolrow\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mappend(nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[0;32m     89\u001b[0m             PreNorm(dim, Residual(Attention(dim, heads \u001b[38;5;241m=\u001b[39m heads, dim_head \u001b[38;5;241m=\u001b[39m dim_head, dropout \u001b[38;5;241m=\u001b[39m attn_dropout))),\n\u001b[0;32m     90\u001b[0m             PreNorm(dim, Residual(FeedForward(dim, dropout \u001b[38;5;241m=\u001b[39m ff_dropout))),\n\u001b[0;32m     91\u001b[0m             PreNorm(dim\u001b[38;5;241m*\u001b[39mnfeats, Residual(Attention(dim\u001b[38;5;241m*\u001b[39mnfeats, heads \u001b[38;5;241m=\u001b[39m heads, dim_head \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m, dropout \u001b[38;5;241m=\u001b[39m attn_dropout))),\n\u001b[1;32m---> 92\u001b[0m             PreNorm(dim\u001b[38;5;241m*\u001b[39mnfeats, Residual(FeedForward(dim\u001b[38;5;241m*\u001b[39mnfeats, dropout \u001b[38;5;241m=\u001b[39m ff_dropout))),\n\u001b[0;32m     93\u001b[0m         ]))\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mappend(nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[0;32m     96\u001b[0m             PreNorm(dim\u001b[38;5;241m*\u001b[39mnfeats, Residual(Attention(dim\u001b[38;5;241m*\u001b[39mnfeats, heads \u001b[38;5;241m=\u001b[39m heads, dim_head \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m, dropout \u001b[38;5;241m=\u001b[39m attn_dropout))),\n\u001b[0;32m     97\u001b[0m             PreNorm(dim\u001b[38;5;241m*\u001b[39mnfeats, Residual(FeedForward(dim\u001b[38;5;241m*\u001b[39mnfeats, dropout \u001b[38;5;241m=\u001b[39m ff_dropout))),\n\u001b[0;32m     98\u001b[0m         ]))\n",
      "Cell \u001b[1;32mIn[7], line 44\u001b[0m, in \u001b[0;36mFeedForward.__init__\u001b[1;34m(self, dim, mult, dropout)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dim, mult \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m, dropout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m):\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m     41\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLinear(dim, dim \u001b[38;5;241m*\u001b[39m mult \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m     42\u001b[0m         GEGLU(),\n\u001b[0;32m     43\u001b[0m         nn\u001b[38;5;241m.\u001b[39mDropout(dropout),\n\u001b[1;32m---> 44\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLinear(dim \u001b[38;5;241m*\u001b[39m mult, dim)\n\u001b[0;32m     45\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Trey\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:112\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[1;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_parameters()\n",
      "File \u001b[1;32mc:\\Users\\Trey\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:118\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m     init\u001b[38;5;241m.\u001b[39mkaiming_uniform_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, a\u001b[38;5;241m=\u001b[39mmath\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[1;32mc:\\Users\\Trey\\anaconda3\\Lib\\site-packages\\torch\\nn\\init.py:518\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[1;34m(tensor, a, mode, nonlinearity, generator)\u001b[0m\n\u001b[0;32m    516\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39muniform_(\u001b[38;5;241m-\u001b[39mbound, bound, generator\u001b[38;5;241m=\u001b[39mgenerator)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "modelsave_path = os.path.join(os.getcwd(),opt.savemodelroot,opt.dataset_name,opt.target_label)\n",
    "if opt.task == 'regression':\n",
    "    opt.dtask = 'reg'\n",
    "else:\n",
    "    opt.dtask = 'clf'\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device is {device}.\")\n",
    "\n",
    "torch.manual_seed(opt.set_seed)\n",
    "os.makedirs(modelsave_path, exist_ok=True)\n",
    "   \n",
    "\n",
    "print('Downloading and processing the dataset, it might take some time.')\n",
    "cat_dims, cat_idxs, con_idxs, X_train, y_train, X_valid, y_valid, X_test, y_test, train_mean, train_std = data_prep(opt.task, datasplit=[.65, .15, .2])\n",
    "continuous_mean_std = np.array([train_mean,train_std]).astype(np.float32) \n",
    "\n",
    "##### Setting some hyperparams based on inputs and dataset\n",
    "_,nfeat = X_train['data'].shape\n",
    "if nfeat > 100:\n",
    "    opt.embedding_size = min(8,opt.embedding_size)\n",
    "    opt.batchsize = min(64, opt.batchsize)\n",
    "if opt.attentiontype != 'col':\n",
    "    opt.transformer_depth = 1\n",
    "    opt.attention_heads = min(4,opt.attention_heads)\n",
    "    opt.attention_dropout = 0.8\n",
    "    opt.embedding_size = min(32,opt.embedding_size)\n",
    "    opt.ff_dropout = 0.8\n",
    "\n",
    "print(nfeat,opt.batchsize)\n",
    "print(opt)\n",
    "\n",
    "train_ds = DataSetCatCon(X_train, y_train, cat_idxs,opt.dtask,continuous_mean_std)\n",
    "trainloader = DataLoader(train_ds, batch_size=opt.batchsize, shuffle=True,num_workers=4)\n",
    "\n",
    "valid_ds = DataSetCatCon(X_valid, y_valid, cat_idxs,opt.dtask, continuous_mean_std)\n",
    "validloader = DataLoader(valid_ds, batch_size=opt.batchsize, shuffle=False,num_workers=4)\n",
    "\n",
    "test_ds = DataSetCatCon(X_test, y_test, cat_idxs,opt.dtask, continuous_mean_std)\n",
    "testloader = DataLoader(test_ds, batch_size=opt.batchsize, shuffle=False,num_workers=4)\n",
    "if opt.task == 'regression':\n",
    "    y_dim = 1\n",
    "else:\n",
    "    y_dim = len(np.unique(y_train['data'][:,0]))\n",
    "\n",
    "cat_dims = np.append(np.array([1]),np.array(cat_dims)).astype(int) #Appending 1 for CLS token, this is later used to generate embeddings.\n",
    "\n",
    "\n",
    "\n",
    "model = SAINT(\n",
    "categories = tuple(cat_dims), \n",
    "num_continuous = len(con_idxs),                \n",
    "dim = opt.embedding_size,                           \n",
    "dim_out = 1,                       \n",
    "depth = opt.transformer_depth,                       \n",
    "heads = opt.attention_heads,                         \n",
    "attn_dropout = opt.attention_dropout,             \n",
    "ff_dropout = opt.ff_dropout,                  \n",
    "mlp_hidden_mults = (4, 2),       \n",
    "cont_embeddings = opt.cont_embeddings,\n",
    "attentiontype = opt.attentiontype,\n",
    "final_mlp_style = opt.final_mlp_style,\n",
    "y_dim = y_dim\n",
    ")\n",
    "vision_dset = opt.vision_dset\n",
    "\n",
    "if y_dim == 2 and opt.task == 'binary':\n",
    "    # opt.task = 'binary'\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "elif y_dim > 2 and  opt.task == 'multiclass':\n",
    "    # opt.task = 'multiclass'\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "elif opt.task == 'regression':\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "else:\n",
    "    raise'case not written yet'\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "if opt.pretrain:\n",
    "    model = SAINT_pretrain(model, cat_idxs,X_train,y_train, continuous_mean_std, opt,device)\n",
    "\n",
    "## Choosing the optimizer\n",
    "\n",
    "if opt.optimizer == 'SGD':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=opt.lr,\n",
    "                          momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = get_scheduler(opt, optimizer)\n",
    "elif opt.optimizer == 'Adam':\n",
    "    optimizer = optim.Adam(model.parameters(),lr=opt.lr)\n",
    "elif opt.optimizer == 'AdamW':\n",
    "    optimizer = optim.AdamW(model.parameters(),lr=opt.lr)\n",
    "best_valid_auroc = 0\n",
    "best_valid_accuracy = 0\n",
    "best_test_auroc = 0\n",
    "best_test_accuracy = 0\n",
    "best_valid_rmse = 100000\n",
    "print('Training begins now.')\n",
    "for epoch in range(opt.epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "        # x_categ is the the categorical data, x_cont has continuous data, y_gts has ground truth ys. cat_mask is an array of ones same shape as x_categ and an additional column(corresponding to CLS token) set to 0s. con_mask is an array of ones same shape as x_cont. \n",
    "        x_categ, x_cont, y_gts, cat_mask, con_mask = data[0].to(device), data[1].to(device),data[2].to(device),data[3].to(device),data[4].to(device)\n",
    "\n",
    "        # We are converting the data to embeddings in the next step\n",
    "        _ , x_categ_enc, x_cont_enc = embed_data_mask(x_categ, x_cont, cat_mask, con_mask,model,vision_dset)           \n",
    "        reps = model.transformer(x_categ_enc, x_cont_enc)\n",
    "        # select only the representations corresponding to CLS token and apply mlp on it in the next step to get the predictions.\n",
    "        y_reps = reps[:,0,:]\n",
    "        \n",
    "        y_outs = model.mlpfory(y_reps)\n",
    "        if opt.task == 'regression':\n",
    "            loss = criterion(y_outs,y_gts) \n",
    "        else:\n",
    "            loss = criterion(y_outs,y_gts.squeeze()) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if opt.optimizer == 'SGD':\n",
    "            scheduler.step()\n",
    "        running_loss += loss.item()\n",
    "    # print(running_loss)\n",
    "    if epoch%5==0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                if opt.task in ['binary','multiclass']:\n",
    "                    accuracy, auroc = classification_scores(model, validloader, device, opt.task,vision_dset)\n",
    "                    test_accuracy, test_auroc = classification_scores(model, testloader, device, opt.task,vision_dset)\n",
    "\n",
    "                    print('[EPOCH %d] VALID ACCURACY: %.3f, VALID AUROC: %.3f' %\n",
    "                        (epoch + 1, accuracy,auroc ))\n",
    "                    print('[EPOCH %d] TEST ACCURACY: %.3f, TEST AUROC: %.3f' %\n",
    "                        (epoch + 1, test_accuracy,test_auroc ))\n",
    "                    if opt.task =='multiclass':\n",
    "                        if accuracy > best_valid_accuracy:\n",
    "                            best_valid_accuracy = accuracy\n",
    "                            best_test_auroc = test_auroc\n",
    "                            best_test_accuracy = test_accuracy\n",
    "                            torch.save(model.state_dict(),'%s/bestmodel.pth' % (modelsave_path))\n",
    "                    else:\n",
    "                        if accuracy > best_valid_accuracy:\n",
    "                            best_valid_accuracy = accuracy\n",
    "                        # if auroc > best_valid_auroc:\n",
    "                        #     best_valid_auroc = auroc\n",
    "                            best_test_auroc = test_auroc\n",
    "                            best_test_accuracy = test_accuracy               \n",
    "                            torch.save(model.state_dict(),'%s/bestmodel.pth' % (modelsave_path))\n",
    "\n",
    "                else:\n",
    "                    valid_rmse = mean_sq_error(model, validloader, device,vision_dset)    \n",
    "                    test_rmse = mean_sq_error(model, testloader, device,vision_dset)  \n",
    "                    print('[EPOCH %d] VALID RMSE: %.3f' %\n",
    "                        (epoch + 1, valid_rmse ))\n",
    "                    print('[EPOCH %d] TEST RMSE: %.3f' %\n",
    "                        (epoch + 1, test_rmse ))\n",
    "                    if valid_rmse < best_valid_rmse:\n",
    "                        best_valid_rmse = valid_rmse\n",
    "                        best_test_rmse = test_rmse\n",
    "                        torch.save(model.state_dict(),'%s/bestmodel.pth' % (modelsave_path))\n",
    "            model.train()\n",
    "                \n",
    "\n",
    "\n",
    "total_parameters = count_parameters(model)\n",
    "print('TOTAL NUMBER OF PARAMS: %d' %(total_parameters))\n",
    "if opt.task =='binary':\n",
    "    print('AUROC on best model:  %.3f' %(best_test_auroc))\n",
    "elif opt.task =='multiclass':\n",
    "    print('Accuracy on best model:  %.3f' %(best_test_accuracy))\n",
    "else:\n",
    "    print('RMSE on best model:  %.3f' %(best_test_rmse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
