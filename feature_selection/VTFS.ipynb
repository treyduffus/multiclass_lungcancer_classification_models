{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group 33, Florida Atlantic University\n",
    "# Implementation of Wangyang Ying, Dongjie Wang, Haifeng Chen, and Yanjie Fu. 2024. Feature Selection as Deep Sequential Generative Learning. 1, 1 (March 2024),\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "import copy\n",
    "import argparse\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from typing import List\n",
    "from collections import namedtuple\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import RidgeClassifier, Ridge, Lasso, LogisticRegression\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.svm import LinearSVC, LinearSVR\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = [\n",
    "    'train_controller.py',          \n",
    "    '--task_name', 'miRNA',\n",
    "    '--method_name', 'transformerVae',\n",
    "    '--top_k', '5',\n",
    "    '--pre_train', 'True',\n",
    "]\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# Basic model parameters.\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=42)\n",
    "parser.add_argument('--new_gen', type=int, default=200)\n",
    "parser.add_argument('--method_name', type=str, choices=['rnn', 'transformer', 'transformerVae'], default='rnn')\n",
    "parser.add_argument('--task_name', type=str, choices=['methelyene', 'miRNA'], default='miRNA')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='used gpu')\n",
    "parser.add_argument('--fe', type=str, choices=['+', '', '-'], default='-')\n",
    "parser.add_argument('--top_k', type=int, default=25)\n",
    "parser.add_argument('--gen_num', type=int, default=25)\n",
    "parser.add_argument('--encoder_layers', type=int, default=1)\n",
    "parser.add_argument('--encoder_hidden_size', type=int, default=64)\n",
    "parser.add_argument('--encoder_emb_size', type=int, default=32)\n",
    "parser.add_argument('--mlp_layers', type=int, default=2)\n",
    "parser.add_argument('--mlp_hidden_size', type=int, default=200)\n",
    "parser.add_argument('--decoder_layers', type=int, default=1)\n",
    "parser.add_argument('--decoder_hidden_size', type=int, default=64)\n",
    "# parser.add_argument('--source_length', type=int, default=40)\n",
    "# parser.add_argument('--encoder_length', type=int, default=20)\n",
    "# parser.add_argument('--decoder_length', type=int, default=40)\n",
    "parser.add_argument('--encoder_dropout', type=float, default=0)\n",
    "parser.add_argument('--mlp_dropout', type=float, default=0)\n",
    "parser.add_argument('--decoder_dropout', type=float, default=0)\n",
    "parser.add_argument('--l2_reg', type=float, default=0.0)\n",
    "# parser.add_argument('--encoder_vocab_size', type=int, default=12)\n",
    "# parser.add_argument('--decoder_vocab_size', type=int, default=12)\n",
    "parser.add_argument('--max_step_size', type=int, default=100)\n",
    "parser.add_argument('--trade_off', type=float, default=0.8)\n",
    "parser.add_argument('--epochs', type=int, default=200)\n",
    "parser.add_argument('--batch_size', type=int, default=32)\n",
    "parser.add_argument('--lr', type=float, default=0.001)\n",
    "parser.add_argument('--optimizer', type=str, default='adam')\n",
    "parser.add_argument('--grad_bound', type=float, default=5.0)\n",
    "\n",
    "parser.add_argument('--transformer_encoder_layers', type=int, default=2)\n",
    "parser.add_argument('--encoder_nhead', type=int, default=8)\n",
    "parser.add_argument('--encoder_embedding_size', type=int, default=64)\n",
    "parser.add_argument('--transformer_encoder_dropout', type=float, default=0.1)\n",
    "parser.add_argument('--transformer_encoder_activation', type=str, default='relu')\n",
    "parser.add_argument('--encoder_dim_feedforward', type=int, default=128)\n",
    "parser.add_argument('--batch_first', type=bool, default=True)  \n",
    "parser.add_argument('--d_latent_dim', type=int, default=64)\n",
    "\n",
    "parser.add_argument('--transformer_decoder_layers', type=int, default=2)\n",
    "parser.add_argument('--decoder_nhead', type=int, default=8)\n",
    "parser.add_argument('--transformer_decoder_dropout', type=float, default=0.1)\n",
    "parser.add_argument('--transformer_decoder_activation', type=str, default='relu')\n",
    "parser.add_argument('--decoder_dim_feedforward', type=int, default=128)\n",
    "parser.add_argument('--decoder_embedding_size', type=int, default=64) \n",
    "parser.add_argument('--pre_train', type=str, default=\"True\") \n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(msg):\n",
    "    logging.error(msg)\n",
    "    print('ERROR: ', msg)\n",
    "\n",
    "def info(msg):\n",
    "    logging.info(msg)\n",
    "    print('INFO: ', msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_absolute_error(y_test, y_predict):\n",
    "    y_test = np.array(y_test)\n",
    "    y_predict = np.array(y_predict)\n",
    "    error = np.sum(np.abs(y_test - y_predict)) / np.sum(np.abs(np.mean(\n",
    "        y_test) - y_test))\n",
    "    return error\n",
    "\n",
    "def test_task_new(Dg, task='cls', init_seed=0):\n",
    "    X = Dg.iloc[:, :-1]\n",
    "    y = Dg.iloc[:, -1].astype(float)\n",
    "    if task == 'mcls':\n",
    "        clf = OneVsRestClassifier(RandomForestClassifier(random_state=0))\n",
    "        pre_list, rec_list, f1_list, maf1_list = [], [], [], []\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                            random_state=init_seed, shuffle=True)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_predict = clf.predict(X_test)\n",
    "        pre_list.append(precision_score(y_test, y_predict, average='macro'))\n",
    "        rec_list.append(recall_score(y_test, y_predict, average='macro'))\n",
    "        f1_list.append(f1_score(y_test, y_predict, average='micro'))\n",
    "        maf1_list.append(f1_score(y_test, y_predict, average='macro'))\n",
    "        return np.mean(pre_list), np.mean(rec_list), np.mean(f1_list), np.mean(maf1_list)\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def downstream_task_new(data, task_type):\n",
    "    X = data.iloc[:, :-1]\n",
    "    y = data.iloc[:, -1].astype(float)\n",
    "    if task_type == 'mcls':\n",
    "        clf = OneVsRestClassifier(RandomForestClassifier(random_state=0, n_jobs=128))\n",
    "        pre_list, rec_list, f1_list, auc_roc_score = [], [], [], []\n",
    "        skf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n",
    "        for train, test in skf.split(X, y):\n",
    "            X_train, y_train, X_test, y_test = X.iloc[train, :], y.iloc[train], X.iloc[test, :], y.iloc[test]\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_predict = clf.predict(X_test)\n",
    "            f1_list.append(f1_score(y_test, y_predict, average='micro'))\n",
    "        return np.mean(f1_list)\n",
    "    else:\n",
    "            return -1\n",
    "        \n",
    "\n",
    "def downstream_task_by_method_std(data, task_type, method):\n",
    "    X = data.iloc[:, :-1]\n",
    "    y = data.iloc[:, -1].astype(float)\n",
    "    if method == 'RF':\n",
    "        if task_type == 'cls':\n",
    "            model = RandomForestClassifier(random_state=0, n_jobs=128)\n",
    "        elif task_type == 'mcls':\n",
    "            model = OneVsRestClassifier(RandomForestClassifier(random_state=0), n_jobs=128)\n",
    "        else:\n",
    "            model = RandomForestRegressor(random_state=0, n_jobs=128)\n",
    "    elif method == 'XGB':\n",
    "        if task_type == 'cls':\n",
    "            model = XGBClassifier(eval_metric='logloss', n_jobs=128)\n",
    "        elif task_type == 'mcls':\n",
    "            model = OneVsRestClassifier(XGBClassifier(eval_metric='logloss'), n_jobs=128)\n",
    "        else:\n",
    "            model = XGBRegressor(eval_metric='logloss', n_jobs=128)\n",
    "    elif method == 'SVM':\n",
    "        if task_type == 'cls':\n",
    "            model = LinearSVC()\n",
    "        elif task_type == 'mcls':\n",
    "            model = LinearSVC()\n",
    "        else:\n",
    "            model = LinearSVR()\n",
    "    elif method == 'KNN':\n",
    "        if task_type == 'cls':\n",
    "            model = KNeighborsClassifier(n_jobs=128)\n",
    "        elif task_type == 'mcls':\n",
    "            model = OneVsRestClassifier(KNeighborsClassifier(), n_jobs=128)\n",
    "        else:\n",
    "            model = KNeighborsRegressor(n_jobs=128)\n",
    "    elif method == 'Ridge':\n",
    "        if task_type == 'cls':\n",
    "            model = RidgeClassifier()\n",
    "        elif task_type == 'mcls':\n",
    "            model = OneVsRestClassifier(RidgeClassifier(), n_jobs=128)\n",
    "        else:\n",
    "            model = Ridge()\n",
    "    elif method == 'LASSO':\n",
    "        if task_type == 'cls':\n",
    "            model = LogisticRegression(penalty='l1',solver='liblinear', n_jobs=128)\n",
    "        elif task_type == 'mcls':\n",
    "            model = OneVsRestClassifier(LogisticRegression(penalty='l1',solver='liblinear'), n_jobs=128)\n",
    "        else:\n",
    "            model = Lasso()\n",
    "    else:  # dt\n",
    "        if task_type == 'cls':\n",
    "            model = DecisionTreeClassifier()\n",
    "        elif task_type == 'mcls':\n",
    "            model = OneVsRestClassifier(DecisionTreeClassifier(), n_jobs=128)\n",
    "        else:\n",
    "            model = DecisionTreeRegressor()\n",
    "\n",
    "    if task_type == 'mcls':\n",
    "        clf = OneVsRestClassifier(RandomForestClassifier(random_state=0))\n",
    "        pre_list, rec_list, f1_list, auc_roc_score = [], [], [], []\n",
    "        skf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n",
    "        for train, test in skf.split(X, y):\n",
    "            X_train, y_train, X_test, y_test = X.iloc[train, :], y.iloc[train], X.iloc[test, :], y.iloc[test]\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_predict = clf.predict(X_test)\n",
    "            f1_list.append(f1_score(y_test, y_predict, average='micro'))\n",
    "        return np.mean(f1_list), np.std(f1_list)\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgrageMeter(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.cnt = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.sum += val * n\n",
    "        self.cnt += n\n",
    "        self.avg = self.sum / self.cnt\n",
    "      \n",
    "def pairwise_accuracy(la, lb):\n",
    "    print(la)\n",
    "    n = len(la)\n",
    "    assert n == len(lb)\n",
    "    total = 0\n",
    "    count = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if la[i] >= la[j] and lb[i] >= lb[j]:\n",
    "                count += 1\n",
    "            if la[i] < la[j] and lb[i] < lb[j]:\n",
    "                count += 1\n",
    "            total += 1\n",
    "    return float(count) / total\n",
    "\n",
    "def hamming_distance(la, lb):\n",
    "    N = len(la)\n",
    "    assert N == len(lb)\n",
    "  \n",
    "    def _hamming_distance(s1, s2):\n",
    "        n = len(s1)\n",
    "        assert n == len(s2)\n",
    "        c = 0\n",
    "        for i, j in zip(s1, s2):\n",
    "            if i != j:\n",
    "                c += 1\n",
    "        return c\n",
    "  \n",
    "    dis = 0\n",
    "    for i in range(N):\n",
    "        line1 = la[i]\n",
    "        line2 = lb[i]\n",
    "        dis += _hamming_distance(line1, line2)\n",
    "    return dis / N\n",
    "\n",
    "def count_parameters_in_MB(model):\n",
    "    return np.sum(np.prod(v.size()) for name, v in model.named_parameters() if \"auxiliary\" not in name)/1e6\n",
    "\n",
    "class FSDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, targets=None, train=True, sos_id=-1, eos_id=-1):\n",
    "        super(FSDataset, self).__init__()\n",
    "        if targets is not None:\n",
    "            assert len(inputs) == len(targets)\n",
    "        self.inputs = copy.deepcopy(inputs)\n",
    "        self.targets = copy.deepcopy(targets)\n",
    "        self.train = train\n",
    "        self.sos_id = sos_id\n",
    "        self.eos_id = eos_id\n",
    "        # self.swap = swap\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        encoder_input = self.inputs[index]\n",
    "        encoder_target = None\n",
    "        if self.targets is not None:\n",
    "            encoder_target = self.targets[index]\n",
    "        encoder_input[encoder_input==-1] = self.eos_id\n",
    "        # if self.swap:\n",
    "        #     a = np.random.randint(0, 5)\n",
    "        #     b = np.random.randint(0, 5)\n",
    "        #     encoder_input = encoder_input[:4 * a] + encoder_input[4 * a + 2:4 * a + 4] + \\\n",
    "        #                     encoder_input[4 * a:4 * a + 2] + encoder_input[4 * (a + 1):20 + 4 * b] + \\\n",
    "        #                     encoder_input[20 + 4 * b + 2:20 + 4 * b + 4] + encoder_input[20 + 4 * b:20 + 4 * b + 2] + \\\n",
    "        #                     encoder_input[20 + 4 * (b + 1):]\n",
    "\n",
    "        if self.train:\n",
    "            decoder_input = torch.cat((torch.tensor([self.sos_id]), encoder_input[:-1]))\n",
    "            sample = {\n",
    "                'encoder_input': encoder_input.long(),\n",
    "                'encoder_target': encoder_target,\n",
    "                'decoder_input': decoder_input.long(),\n",
    "                'decoder_target': encoder_input.long(),\n",
    "            }\n",
    "        else:\n",
    "            sample = {\n",
    "                'encoder_input': encoder_input.long(),\n",
    "                'decoder_target': encoder_input.long(),\n",
    "            }\n",
    "            if encoder_target is not None:\n",
    "                sample['encoder_target'] = encoder_target\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Record(object):\n",
    "    def __init__(self, operation, performance):\n",
    "        if isinstance(operation, List):\n",
    "            self.operation = np.array(operation)\n",
    "        elif isinstance(operation, torch.Tensor):\n",
    "            self.operation = operation.numpy()\n",
    "        else:\n",
    "            assert isinstance(operation, np.ndarray)\n",
    "            self.operation = operation\n",
    "        self.performance = performance\n",
    "\n",
    "    def get_permutated(self):\n",
    "        pass\n",
    "\n",
    "    def get_ordered(self):\n",
    "        pass\n",
    "\n",
    "    def repeat(self):\n",
    "        pass\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, Record):\n",
    "            return False\n",
    "        return self.__hash__() == other.__hash__()\n",
    "\n",
    "    def __hash__(self):\n",
    "        return str(self.operation).__hash__()\n",
    "\n",
    "class SelectionRecord(Record):\n",
    "    def __init__(self, operation, performance):\n",
    "        super().__init__(operation, performance)\n",
    "        self.max_size = operation.shape[0]\n",
    "\n",
    "    def _get_ordered(self):\n",
    "        indice_select = torch.arange(0, self.max_size)[self.operation == 1]\n",
    "        return indice_select, torch.FloatTensor([self.performance])\n",
    "\n",
    "    def get_permutated(self, num=25, padding=True, padding_value=-1):\n",
    "        ordered, performance = self._get_ordered()\n",
    "        size = ordered.shape[0]\n",
    "        shuffled_indices = torch.empty(num + 1, size)\n",
    "        shuffled_indices[0] = ordered\n",
    "        label = performance.unsqueeze(0).repeat(num + 1, 1)\n",
    "        for i in range(num):\n",
    "            shuffled_indices[i + 1] = ordered[torch.randperm(size)]\n",
    "        if padding and size < self.max_size:\n",
    "            shuffled_indices = F.pad(shuffled_indices, (0, (self.max_size - size)), 'constant', padding_value)\n",
    "        return shuffled_indices, label\n",
    "\n",
    "    def repeat(self, num=25, padding=True, padding_value=-1):\n",
    "        ordered, performance = self._get_ordered()\n",
    "        size = ordered.shape[0]\n",
    "        label = performance.unsqueeze(0).repeat(num + 1, 1)\n",
    "        indices = ordered.unsqueeze(0).repeat(num+1, 1)\n",
    "        if padding and size < self.max_size:\n",
    "            indices = F.pad(indices, (0, (self.max_size - size)), 'constant', padding_value)\n",
    "            return indices, label\n",
    "\n",
    "\n",
    "class RecordList(object):\n",
    "    def __init__(self):\n",
    "        self.r_list = set()\n",
    "\n",
    "    def append(self, op, val):\n",
    "        self.r_list.add(SelectionRecord(op, val))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.r_list)\n",
    "\n",
    "    def generate(self, num=25, padding=True, padding_value=-1):\n",
    "        results = []\n",
    "        labels = []\n",
    "        for record in self.r_list:\n",
    "            result, label = record.get_permutated(num, padding, padding_value)\n",
    "            results.append(result)\n",
    "            labels.append(label)\n",
    "\n",
    "        return torch.cat(results, 0), torch.cat(labels, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Enviroment\n",
    "## Generates different feature sequences for model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trey\\OneDrive - Florida Atlantic University\\source\\cancer_class\\multiclass_lungcancer_classification_models\n"
     ]
    }
   ],
   "source": [
    "base_path = os.getcwd()\n",
    "\n",
    "base_path = os.path.dirname(base_path)\n",
    "\n",
    "print(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_miRNA_data = pd.read_csv('../processed_data/miRNA_stage_subtype.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating data from labels\n",
    "pos_neg_labels, stage_labels, subtype_labels = labeled_miRNA_data.iloc[:, -2], labeled_miRNA_data.iloc[:, -2], labeled_miRNA_data.iloc[:, -1]\n",
    "\n",
    "# Condensing stage information into a general diagnosis\n",
    "# 0 == negative, 1 == posistive for lung cancer\n",
    "pos_neg_labels = pos_neg_labels.apply(lambda x: 1 if x > 1 else x)\n",
    "\n",
    "miRNA_data = labeled_miRNA_data.iloc[:, : -2]\n",
    "miRNA_data['label'] = pos_neg_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:  initialize the train and test dataset\n",
      "INFO:  current dataset : miRNA\n",
      "INFO:  the size of shape is : 1882\n",
      "INFO:  original performance is : 0.9899208244313185\n",
      "INFO:  training on overall eval cost : 0.8734791278839111s\n",
      "INFO:  RF\n",
      "INFO:  training on RF eval cost : 2.0530478954315186s\n",
      "INFO:  XGB\n",
      "INFO:  training on XGB eval cost : 2.0559518337249756s\n",
      "INFO:  SVM\n",
      "INFO:  training on SVM eval cost : 2.0496556758880615s\n",
      "INFO:  KNN\n",
      "INFO:  training on KNN eval cost : 2.0814359188079834s\n",
      "INFO:  Ridge\n",
      "INFO:  training on Ridge eval cost : 2.0381853580474854s\n",
      "INFO:  DT\n",
      "INFO:  training on DT eval cost : 2.0244383811950684s\n",
      "INFO:  LASSO\n",
      "INFO:  training on LASSO eval cost : 2.022604465484619s\n",
      "1\n",
      "Record: <__main__.SelectionRecord object at 0x000001CA6E617260>, Label: tensor([[0.9899]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trey\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:  Pre on original is: 0.4914, Pre on generated is: 0.4914\n",
      "INFO:  Rec on original is: 0.5000, Rec on generated is: 0.5000\n",
      "INFO:  Micro-F1 on original is: 0.9829, Micro-F1 on generated is: 0.9829\n",
      "INFO:  Macro-F1 on original is: 0.4957, Macro-F1 on generated is: 0.4957\n",
      "INFO:  save the records...\n",
      "INFO:  save the choice to c:\\Users\\Trey\\OneDrive - Florida Atlantic University\\source\\cancer_class\\multiclass_lungcancer_classification_models/history/miRNA/choice.pt\n",
      "INFO:  save the performance to c:\\Users\\Trey\\OneDrive - Florida Atlantic University\\source\\cancer_class\\multiclass_lungcancer_classification_models/history/miRNA/performance.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trey\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "MEASUREMENT = {\n",
    "    'mcls' : ['precision', 'recall', 'mif1', 'maf1']\n",
    "}\n",
    "\n",
    "model_performance = {\n",
    "    'mcls':namedtuple('ModelPerformance', MEASUREMENT['mcls']),\n",
    "}\n",
    "\n",
    "class Evaluator(object):\n",
    "    def __init__(self, task, task_type=None, dataset=None):\n",
    "        self.original_report = None\n",
    "        self.records = RecordList()\n",
    "        self.task_name = task\n",
    "        if task_type is None:\n",
    "            # self.task_type = TASK_DICT[self.task_name]\n",
    "             self.task_type = 'mcls'\n",
    "        else:\n",
    "            self.task_type = task_type\n",
    "\n",
    "        if task == \"miRNA\":\n",
    "            original = miRNA_data\n",
    "        elif dataset is None:\n",
    "            data_path = os.path.join(base_path, \"processed_data\", self.task_name + \"_stage_subtype.csv\")\n",
    "            original = pd.read_csv(data_path)\n",
    "        else:\n",
    "            original = dataset\n",
    "        col = np.arange(original.shape[1])\n",
    "        self.col_names = original.columns\n",
    "        original.columns = col\n",
    "        y = original.iloc[:, -1]\n",
    "        x = original.iloc[:, :-1]\n",
    "        self.original = original.fillna(value=0)\n",
    "        y = self.original.iloc[:, -1]\n",
    "        x = self.original.iloc[:, :-1]\n",
    "        # 80%数据用于构建embedding space 另外20%用于测试\n",
    "        X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2,\n",
    "                                                            random_state=0, shuffle=True)\n",
    "\n",
    "        self.train = pd.concat([pd.DataFrame(X_train), pd.DataFrame(y_train)], axis=1)\n",
    "        self.test = pd.concat([pd.DataFrame(X_test), pd.DataFrame(y_test)], axis=1)\n",
    "        info('initialize the train and test dataset')\n",
    "        self._check_path()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n",
    "    def generate_data(self, operation, flag):\n",
    "        pass\n",
    "\n",
    "    def get_performance(self, data=None):\n",
    "        if data is None:\n",
    "            data = self.original\n",
    "        return downstream_task_new(data, self.task_type)\n",
    "\n",
    "    def report_ds(self):\n",
    "        pass\n",
    "\n",
    "    def _store_history(self, choice, performance):\n",
    "        self.records.append(choice, performance)\n",
    "\n",
    "    def _flush_history(self, choices, performances, is_permuted, num, padding):\n",
    "        if is_permuted:\n",
    "            flag_1 = 'augmented'\n",
    "        else:\n",
    "            flag_1 = 'original'\n",
    "        if padding:\n",
    "            flag_2 = 'padded'\n",
    "        else:\n",
    "            flag_2 = 'not_padded'\n",
    "        torch.save(choices, f'{base_path}/history/{self.task_name}/choice.{flag_1}.{flag_2}.{num}.pt')\n",
    "        info(f'save the choice to {base_path}/history/{self.task_name}/choice.pt')\n",
    "        torch.save(performances, f'{base_path}/history/{self.task_name}/performance.{flag_1}.{flag_2}.{num}.pt')\n",
    "        info(f'save the performance to {base_path}/history/{self.task_name}/performance.pt')\n",
    "\n",
    "    def _check_path(self):\n",
    "        if not os.path.exists(f'{base_path}/history/{self.task_name}'):\n",
    "            os.makedirs(f'{base_path}/history/{self.task_name}')\n",
    "\n",
    "    def save(self, num=25, padding=True, padding_value=-1):\n",
    "        if num > 0:\n",
    "            is_permuted = True\n",
    "        else:\n",
    "            is_permuted = False\n",
    "        info('save the records...')\n",
    "        choices, performances = \\\n",
    "            self.records.generate(num=num, padding=padding, padding_value=padding_value)\n",
    "        self._flush_history(choices, performances, is_permuted, num, padding)\n",
    "\n",
    "    def get_record(self, num=0, eos=-1):\n",
    "        results = []\n",
    "        labels = []\n",
    "        print(len(self.records.r_list))\n",
    "        for record in self.records.r_list:\n",
    "            result, label = record.get_permutated(num, True, eos)\n",
    "            print(f\"Record: {record}, Label: {label}\")\n",
    "            results.append(result)\n",
    "            labels.append(label)\n",
    "        # print(result)\n",
    "        # print(labels)\n",
    "        return torch.cat(results, 0), torch.cat(labels, 0)\n",
    "\n",
    "    def get_triple_record(self, num=0, eos=-1, mode='ht'):\n",
    "        h_results = []\n",
    "        labels = []\n",
    "        t_results = []\n",
    "        h_seed = []\n",
    "        labels_seed = []\n",
    "        for record in self.records.r_list:\n",
    "            if mode.__contains__('h'):\n",
    "                h, label = record.get_permutated(num, True, eos)\n",
    "            else:\n",
    "                h, label = record.repeat(num, True, eos)\n",
    "            if mode.__contains__('t'):\n",
    "                t, _ = record.get_permutated(num, True, eos)\n",
    "            else:\n",
    "                t, _ = record.repeat(num, True, eos)\n",
    "            h_results.append(h)\n",
    "            t_results.append(t)\n",
    "            labels.append(label)\n",
    "            h_seed.append(h_results[0])\n",
    "            labels_seed.append(labels[0])\n",
    "        return torch.cat(h_results, 0), torch.cat(labels, 0), torch.cat(t_results), \\\n",
    "               torch.cat(h_seed), torch.cat(labels_seed),\n",
    "\n",
    "    def report_performance(self, choice, store=True, rp=True, flag='', init_seed=0):\n",
    "        flag = 'train'\n",
    "        opt_ds = self.generate_data(choice, flag)\n",
    "        a, b, c, d = test_task_new(opt_ds, task=self.task_type, init_seed=init_seed)\n",
    "        report = model_performance[self.task_type](a, b, c, d)\n",
    "        if flag == 'test':\n",
    "            store = False\n",
    "        if self.original_report is None:\n",
    "            a, b, c, d = test_task_new(self.train, task=self.task_type, init_seed=init_seed)\n",
    "            self.original_report = (a, b, c, d)\n",
    "        else:\n",
    "            a, b, c, d = self.original_report\n",
    "        original_report = model_performance[self.task_type](a, b, c, d)\n",
    "        if self.task_type == 'reg':\n",
    "            final_result = report.rae\n",
    "            if rp:\n",
    "                info('1-MAE on original is: {:.4f}, 1-MAE on generated is: {:.4f}'.\n",
    "                     format(original_report.mae, report.mae))\n",
    "                info('1-MSE on original is: {:.4f}, 1-MSE on generated is: {:.4f}'.\n",
    "                     format(original_report.mse, report.mse))\n",
    "                info('1-RAE on original is: {:.4f}, 1-RAE on generated is: {:.4f}'.\n",
    "                     format(original_report.rae, report.rae))\n",
    "                info('1-RMSE on original is: {:.4f}, 1-RMSE on generated is: {:.4f}'.\n",
    "                     format(original_report.rmse, report.rmse))\n",
    "        elif self.task_type == 'cls':\n",
    "            final_result = report.f1_score\n",
    "            if rp:\n",
    "                info('Pre on original is: {:.4f}, Pre on generated is: {:.4f}'.\n",
    "                     format(original_report.precision, report.precision))\n",
    "                info('Rec on original is: {:.4f}, Rec on generated is: {:.4f}'.\n",
    "                     format(original_report.recall, report.recall))\n",
    "                info('F-1 on original is: {:.4f}, F-1 on generated is: {:.4f}'.\n",
    "                     format(original_report.f1_score, report.f1_score))\n",
    "                info('ROC/AUC on original is: {:.4f}, ROC/AUC on generated is: {:.4f}'.\n",
    "                     format(original_report.roc_auc, report.roc_auc))\n",
    "        elif self.task_type == 'det':\n",
    "            final_result = report.ras\n",
    "            if rp:\n",
    "                info(\n",
    "                    'Average Precision Score on original is: {:.4f}, Average Precision Score on generated is: {:.4f}'\n",
    "                    .format(original_report.map, report.map))\n",
    "                info(\n",
    "                    'F1 Score on original is: {:.4f}, F1 Score on generated is: {:.4f}'\n",
    "                    .format(original_report.f1_score, report.f1_score))\n",
    "                info(\n",
    "                    'ROC AUC Score on original is: {:.4f}, ROC AUC Score on generated is: {:.4f}'\n",
    "                    .format(original_report.ras, report.ras))\n",
    "                info(\n",
    "                    'Recall on original is: {:.4f}, Recall Score on generated is: {:.4f}'\n",
    "                    .format(original_report.recall, report.recall))\n",
    "        elif self.task_type == 'mcls':\n",
    "            final_result = report.mif1\n",
    "            if rp:\n",
    "                info('Pre on original is: {:.4f}, Pre on generated is: {:.4f}'.\n",
    "                     format(original_report.precision, report.precision))\n",
    "                info('Rec on original is: {:.4f}, Rec on generated is: {:.4f}'.\n",
    "                     format(original_report.recall, report.recall))\n",
    "                info('Micro-F1 on original is: {:.4f}, Micro-F1 on generated is: {:.4f}'.\n",
    "                     format(original_report.mif1, report.mif1))\n",
    "                info('Macro-F1 on original is: {:.4f}, Macro-F1 on generated is: {:.4f}'.\n",
    "                     format(original_report.maf1, report.maf1))\n",
    "        else:\n",
    "            error('wrong task name!!!!!')\n",
    "            assert False\n",
    "        if store:\n",
    "            self._store_history(choice, final_result)\n",
    "        return final_result\n",
    "\n",
    "\n",
    "class FeatureEvaluator(Evaluator):\n",
    "    def __init__(self, task, task_type=None, dataset=None):\n",
    "        super().__init__(task, task_type, dataset)\n",
    "        self.ds_size = self.original.shape[1] - 1\n",
    "\n",
    "    def generate_data(self, choice, flag=''):\n",
    "        # choice = choice.T\n",
    "        if choice.shape[0] != self.ds_size:\n",
    "            print(choice.shape)\n",
    "            print(self.ds_size)\n",
    "            error('wrong shape of choice')\n",
    "            assert False\n",
    "        if flag == 'test':\n",
    "            ds = self.test\n",
    "        elif flag == 'train':\n",
    "            ds = self.train\n",
    "        else:\n",
    "            ds = self.original\n",
    "        X = ds.iloc[:, :-1]\n",
    "        indice = torch.arange(0, self.ds_size)[choice == 1].numpy() # trying conversion\n",
    "        X = X.iloc[:, indice].astype(np.float64)\n",
    "        y = ds.iloc[:, -1].astype(np.float64)\n",
    "        Dg = pd.concat([pd.DataFrame(X), pd.DataFrame(y)], axis=1)\n",
    "        return Dg\n",
    "\n",
    "    def _full_mask(self):\n",
    "        return torch.FloatTensor([1] * self.ds_size)\n",
    "\n",
    "    def report_ds(self):\n",
    "        per = self.get_performance()\n",
    "        info(f'current dataset : {self.task_name}')\n",
    "        info(f'the size of shape is : {self.original.shape[1]}')\n",
    "        info(f'original performance is : {per}')\n",
    "        self._store_history(self._full_mask(), per)\n",
    "\n",
    "import time\n",
    "if __name__ == '__main__':\n",
    "    fe = FeatureEvaluator(args.task_name)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    fe.report_ds()\n",
    "    end_time = time.time()\n",
    "    info(f'training on overall eval cost : {end_time - start_time}s')\n",
    "    for method in ['RF', 'XGB', 'SVM', 'KNN', 'Ridge', 'DT', 'LASSO']:\n",
    "        info(method)\n",
    "        start_time = time.time()\n",
    "        p, std = downstream_task_by_method_std(fe.original, fe.task_type, method)\n",
    "        # print(p, std)\n",
    "        end_time = time.time()\n",
    "        info(f'training on {method} eval cost : {end_time - start_time}s')\n",
    "    choice, labels = fe.get_record(0, eos=fe.ds_size)\n",
    "    fe.report_performance(choice[0]) # NEED CHOICE TO REPORT PERFORMANCE \n",
    "    # print(labels)\n",
    "    fe.save()\n",
    "    file_path = f'{base_path}/history/{args.task_name}/fe.pkl'\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(fe, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 layers,\n",
    "                 vocab_size,\n",
    "                 hidden_size):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.hidden_size)\n",
    "\n",
    "    def infer(self, x, predict_lambda, direction='-'):\n",
    "        encoder_outputs, encoder_hidden, seq_emb, predict_value, mu, logvar = self(x)\n",
    "        grads_on_outputs = torch.autograd.grad(predict_value, encoder_outputs, torch.ones_like(predict_value))[0]\n",
    "        if direction == '+':\n",
    "            new_encoder_outputs = encoder_outputs + predict_lambda * grads_on_outputs\n",
    "        elif direction == '-':\n",
    "            new_encoder_outputs = encoder_outputs - predict_lambda * grads_on_outputs\n",
    "        else:\n",
    "            raise ValueError('Direction must be + or -, got {} instead'.format(direction))\n",
    "        new_encoder_outputs = F.normalize(new_encoder_outputs, 2, dim=-1)\n",
    "        new_seq_emb = torch.mean(new_encoder_outputs, dim=1)\n",
    "        new_seq_emb = F.normalize(new_seq_emb, 2, dim=-1)\n",
    "        return encoder_outputs, encoder_hidden, seq_emb, predict_value, new_encoder_outputs, new_seq_emb\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # 初始化Shape为(max_len, d_model)的PE (positional encoding)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # 初始化一个tensor [[0, 1, 2, 3, ...]]\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        # 这里就是sin和cos括号中的内容，通过e和ln进行了变换\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        # 计算PE(pos, 2i)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # 计算PE(pos, 2i+1)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # 为了方便计算，在最外面在unsqueeze出一个batch\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # 如果一个参数不参与梯度下降，但又希望保存model的时候将其保存下来\n",
    "        # 这个时候就可以用register_buffer\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x 为embedding后的inputs,例如(1,7, 128),batch size为1,7个单词,单词维度为128\n",
    "        \"\"\"\n",
    "        # 将x和positional encoding相加。\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerEncoderVAE(Encoder):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_encoder_layers,\n",
    "            nhead, \n",
    "            vocab_size,\n",
    "            embedding_size,\n",
    "            dropout, \n",
    "            activation,\n",
    "            dim_feedforward,\n",
    "            batch_first,\n",
    "            mlp_layers,\n",
    "            mlp_hidden_size,\n",
    "            mlp_dropout,\n",
    "            d_latent_dim,\n",
    "            ):\n",
    "        super(TransformerEncoderVAE, self).__init__(num_encoder_layers, vocab_size, embedding_size)\n",
    "        # positional layer\n",
    "        self.positionalEncoding = PositionalEncoding(\n",
    "                                d_model = embedding_size,\n",
    "                                dropout = dropout,\n",
    "                                max_len = vocab_size)\n",
    "        # multi-head attention && feed forward && norm -> encoder layer \n",
    "        self.encoderLayer = nn.TransformerEncoderLayer(\n",
    "                                d_model = embedding_size,\n",
    "                                nhead = nhead,\n",
    "                                dropout = dropout,\n",
    "                                activation = activation,\n",
    "                                dim_feedforward = dim_feedforward,\n",
    "                                batch_first = batch_first)\n",
    "        # stack encoder layers to construct transformer encoder\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "                                encoder_layer = self.encoderLayer,\n",
    "                                num_layers = num_encoder_layers)\n",
    "        self.mu = nn.Linear(embedding_size, d_latent_dim)\n",
    "        self.logvar = nn.Linear(embedding_size, d_latent_dim)\n",
    "        # mlp layer\n",
    "        self.mlp = nn.Sequential()\n",
    "        for i in range(mlp_layers):\n",
    "            if i == 0:\n",
    "                self.mlp.add_module('layer_{}'.format(i), nn.Sequential(\n",
    "                    nn.Linear(d_latent_dim, mlp_hidden_size),\n",
    "                    nn.ReLU(inplace=False),\n",
    "                    nn.Dropout(p=mlp_dropout)))\n",
    "            else:\n",
    "                self.mlp.add_module('layer_{}'.format(i), nn.Sequential(\n",
    "                    nn.Linear(mlp_hidden_size, mlp_hidden_size),\n",
    "                    nn.ReLU(inplace=False),\n",
    "                    nn.Dropout(p=mlp_dropout)))\n",
    "        self.regressor = nn.Linear(d_latent_dim if mlp_layers == 0 else mlp_hidden_size, 1)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # epsilon: 噪声\n",
    "        # epsilon = torch.randn_like(mu)\n",
    "        epsilon = 1\n",
    "        return mu + epsilon * torch.exp(logvar/2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # get embedding\n",
    "        embedding = self.embedding(x)\n",
    "        # add positional information\n",
    "        embedding = self.positionalEncoding(embedding)\n",
    "        \n",
    "        # encoder output\n",
    "        out = self.encoder(embedding)\n",
    "        out = F.normalize(out, 2, dim=-1)\n",
    "        encoder_outputs = out\n",
    "        \n",
    "        # add all embedding and compute mean / summarize\n",
    "        out = torch.mean(out, dim=1)\n",
    "        out = F.normalize(out, 2, dim=-1)\n",
    "        seq_emb = out\n",
    "\n",
    "        # compute mu, logvar to compute the KL-loss\n",
    "        mu, logvar = self.mu(out), self.logvar(out)\n",
    "        # reparameterize\n",
    "        out = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        # evaluator\n",
    "        out = self.mlp(out)\n",
    "        out = self.regressor(out)\n",
    "        predict_value = torch.sigmoid(out)\n",
    "        # 适配RNNEncoder的输出\n",
    "        encoder_hidden = None\n",
    "        # encoder_outputs, encoder_hidden, seq_emb, predict_value\n",
    "        # encoder_outputs shape (batch_size, sequence_length, embedding_dim)\n",
    "        return encoder_outputs, encoder_hidden, seq_emb, predict_value, mu, logvar\n",
    "    \n",
    "\n",
    "def construct_encoder(fe: FeatureEvaluator, args) -> Encoder:\n",
    "    name = args.method_name\n",
    "    size = fe.ds_size\n",
    "    info(f'Construct Encoder with method {name}...')\n",
    "    if name == 'transformerVae':\n",
    "        return TransformerEncoderVAE(\n",
    "            num_encoder_layers = args.transformer_encoder_layers,\n",
    "            nhead = args.encoder_nhead,\n",
    "            vocab_size = size + 1,\n",
    "            embedding_size = args.encoder_embedding_size,\n",
    "            dropout = args.transformer_encoder_dropout,\n",
    "            activation = args.transformer_encoder_activation,\n",
    "            dim_feedforward = args.encoder_dim_feedforward,\n",
    "            batch_first = args.batch_first,\n",
    "            mlp_layers = args.mlp_layers,\n",
    "            mlp_hidden_size = args.mlp_hidden_size,\n",
    "            mlp_dropout = args.encoder_dropout,\n",
    "            d_latent_dim = args.d_latent_dim\n",
    "        )\n",
    "    else:\n",
    "        assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim, source_dim=None, output_dim=None, bias=False):\n",
    "        super(Attention, self).__init__()\n",
    "        if source_dim is None:\n",
    "            source_dim = input_dim\n",
    "        if output_dim is None:\n",
    "            output_dim = input_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.source_dim = source_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.input_proj = nn.Linear(input_dim, source_dim, bias=bias)\n",
    "        self.output_proj = nn.Linear(input_dim + source_dim, output_dim, bias=bias)\n",
    "        self.mask = None\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, input, source_hids):\n",
    "        batch_size = input.size(0)\n",
    "        source_len = source_hids.size(1)\n",
    "\n",
    "        # (batch, tgt_len, input_dim) -> (batch, tgt_len, source_dim)\n",
    "        x = self.input_proj(input)\n",
    "\n",
    "        # (batch, tgt_len, source_dim) * (batch, src_len, source_dim) -> (batch, tgt_len, src_len)\n",
    "        attn = torch.bmm(x, source_hids.transpose(1, 2))\n",
    "        if self.mask is not None:\n",
    "            attn.data.masked_fill_(self.mask, -float('inf'))\n",
    "        attn = F.softmax(attn.view(-1, source_len), dim=1).view(batch_size, -1, source_len)\n",
    "\n",
    "        # (batch, tgt_len, src_len) * (batch, src_len, source_dim) -> (batch, tgt_len, source_dim)\n",
    "        mix = torch.bmm(attn, source_hids)\n",
    "\n",
    "        # concat -> (batch, tgt_len, source_dim + input_dim)\n",
    "        combined = torch.cat((mix, input), dim=2)\n",
    "        # output -> (batch, tgt_len, output_dim)\n",
    "        output = torch.tanh(self.output_proj(combined.view(-1, self.input_dim + self.source_dim))).view(batch_size, -1,\n",
    "                                                                                                        self.output_dim)\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    KEY_ATTN_SCORE = 'attention_score'\n",
    "    KEY_LENGTH = 'length'\n",
    "    KEY_SEQUENCE = 'sequence'\n",
    "\n",
    "    def __init__(self,\n",
    "                 layers,\n",
    "                 vocab_size,\n",
    "                 hidden_size,\n",
    "                 dropout,\n",
    "                 length, gpu):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.length = length  # total length to decode\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.hidden_size)\n",
    "        self.sos_id = vocab_size - 1\n",
    "        self.eos_id = vocab_size - 1\n",
    "        self.gpu = gpu\n",
    "\n",
    "\n",
    "class TransformerDecoder(Decoder):\n",
    "    def __init__(self, \n",
    "                 num_decoder_layers, \n",
    "                 nhead, \n",
    "                 vocab_size, \n",
    "                 embedding_size,\n",
    "                 dropout, \n",
    "                 activation,\n",
    "                 dim_feedforward,\n",
    "                 batch_first,\n",
    "                 length, \n",
    "                 gpu):\n",
    "        super(TransformerDecoder, self).__init__(\n",
    "            num_decoder_layers, \n",
    "            vocab_size, \n",
    "            embedding_size, \n",
    "            dropout, \n",
    "            length, \n",
    "            gpu)\n",
    "        self.embedding_size = embedding_size\n",
    "        # positional layer\n",
    "        self.positionalEncoding = PositionalEncoding(\n",
    "                                d_model = embedding_size,\n",
    "                                dropout = dropout,\n",
    "                                max_len = vocab_size)\n",
    "        # decoder layer\n",
    "        self.decoderLayer = nn.TransformerDecoderLayer(\n",
    "                                d_model = embedding_size,\n",
    "                                nhead = nhead,\n",
    "                                dropout = dropout,\n",
    "                                activation = activation,\n",
    "                                dim_feedforward = dim_feedforward,\n",
    "                                batch_first = batch_first)\n",
    "        # stack decoder layer to construct transformer decoder\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "                                decoder_layer = self.decoderLayer,\n",
    "                                num_layers = num_decoder_layers)\n",
    "        self.attention = Attention(embedding_size)\n",
    "        # out put\n",
    "        self.out = nn.Linear(embedding_size, vocab_size)\n",
    "        \n",
    "    def forward_train_valid(self, x, encoder_outputs):\n",
    "        batch_size = x.shape[0]\n",
    "        output_size = x.shape[1]\n",
    "        # By passing cuda for now \n",
    "        # x = x.cuda(self.gpu)\n",
    "        x.to('cpu')\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.positionalEncoding(embedded)\n",
    "        \n",
    "        # construct squence mask\n",
    "        # tgt_mask = nn.Transformer.generate_square_subsequent_mask(output_size).cuda(self.gpu)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(output_size).to('cpu')\n",
    "        out = self.decoder(embedded, encoder_outputs, tgt_mask)\n",
    "\n",
    "        # out, attn = self.attention(out, encoder_outputs)\n",
    "\n",
    "        predict_softmax = F.log_softmax(self.out(out.contiguous().view(-1, self.embedding_size)), dim=1)\n",
    "        predict_softmax = predict_softmax.view(batch_size, output_size, -1)\n",
    "        # print(predict_softmax)\n",
    "        return predict_softmax\n",
    "\n",
    "    def forward_step(self, encoder_outputs, input_id):\n",
    "        embedded = self.embedding(input_id)\n",
    "        embedded = self.positionalEncoding(embedded)\n",
    "        # tgt_mask = nn.Transformer.generate_square_subsequent_mask(input_id.shape[1]).cuda(self.gpu)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(input_id.shape[1]).to('cpu')\n",
    "        out = self.decoder(embedded, encoder_outputs, tgt_mask)\n",
    "\n",
    "        # out, attn = self.attention(out, encoder_outputs)\n",
    "\n",
    "        predict_softmax = F.log_softmax(self.out(out.contiguous().view(-1, self.embedding_size)), dim=1)\n",
    "        _, next_input_id = predict_softmax.max(dim=1, keepdim=True)\n",
    "        output_id = next_input_id.reshape(input_id.shape[0], input_id.shape[1])\n",
    "        return output_id\n",
    "    \n",
    "    def forward_infer(self, encoder_outputs):\n",
    "        # initialize the input id\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        input_id = torch.LongTensor([self.sos_id] * batch_size).view(batch_size, 1).to('cpu')\n",
    "        # input_id = torch.LongTensor([self.sos_id] * batch_size).view(batch_size, 1).cuda(self.gpu)\n",
    "        for step in range(self.length):\n",
    "            output_id = self.forward_step(encoder_outputs, input_id)\n",
    "            input_id = torch.cat((input_id, output_id[:,-1].reshape(-1, 1)), dim=1)\n",
    "        return output_id\n",
    "    \n",
    "\n",
    "def construct_decoder(fe: FeatureEvaluator, args) -> Decoder:\n",
    "        name = args.method_name\n",
    "        size = fe.ds_size\n",
    "        info(f'Construct Decoder with method {name}...')\n",
    "        if name == 'transformer' or name == 'transformerVae':\n",
    "            return TransformerDecoder(\n",
    "                num_decoder_layers = args.transformer_decoder_layers,\n",
    "                nhead = args.decoder_nhead,\n",
    "                vocab_size = size + 1,\n",
    "                embedding_size = args.decoder_embedding_size,\n",
    "                dropout = args.transformer_decoder_dropout,\n",
    "                activation = args.transformer_decoder_activation,\n",
    "                dim_feedforward = args.decoder_dim_feedforward,\n",
    "                batch_first = args.batch_first,\n",
    "                length = size,\n",
    "                gpu = args.gpu\n",
    "            )\n",
    "        else:\n",
    "            assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_ID = 0\n",
    "EOS_ID = 0\n",
    "\n",
    "\n",
    "# gradient based automatic feature selection\n",
    "class GAFS(nn.Module):\n",
    "    def __init__(self,\n",
    "                 fe:FeatureEvaluator,\n",
    "                 args\n",
    "                 ):\n",
    "        super(GAFS, self).__init__()\n",
    "        self.style = args.method_name\n",
    "        self.gpu = args.gpu\n",
    "        self.encoder = construct_encoder(fe, args)\n",
    "        self.decoder = construct_decoder(fe, args)\n",
    "\n",
    "    def flatten_parameters(self):\n",
    "        self.encoder.rnn.flatten_parameters()\n",
    "        self.decoder.rnn.flatten_parameters()\n",
    "\n",
    "    def forward(self, input_variable, target_variable=None):\n",
    "        mu = 0.0\n",
    "        logvar = 0.0\n",
    "        if self.style == \"transformerVae\":\n",
    "            encoder_outputs, encoder_hidden, feat_emb, predict_value, mu, logvar = self.encoder.forward(input_variable)\n",
    "            decoder_outputs = self.decoder.forward_train_valid(target_variable, encoder_outputs)\n",
    "            _, feat = decoder_outputs.max(2, keepdim=True)\n",
    "            feat = feat.reshape(input_variable.size(0), input_variable.size(1))\n",
    "\n",
    "        return predict_value, decoder_outputs, feat, mu, logvar\n",
    "\n",
    "\n",
    "    def generate_new_feature(self, input_variable, predict_lambda=1, direction='-'):\n",
    "        if self.style == 'transformer' or self.style == 'transformerVae':\n",
    "            encoder_outputs, encoder_hidden, feat_emb, predict_value, new_encoder_outputs, new_feat_emb = \\\n",
    "                self.encoder.infer(input_variable, predict_lambda, direction=direction)\n",
    "            new_feat_seq = self.decoder.forward_infer(new_encoder_outputs)\n",
    "        return new_feat_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Trey\\AppData\\Local\\Temp\\ipykernel_16188\\3761490883.py:52: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return np.sum(np.prod(v.size()) for name, v in model.named_parameters() if \"auxiliary\" not in name)/1e6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:  No GPU found!\n",
      "INFO:  Args = Namespace(seed=42, new_gen=200, method_name='transformerVae', task_name='miRNA', gpu=0, fe='-', top_k=5, gen_num=25, encoder_layers=1, encoder_hidden_size=64, encoder_emb_size=32, mlp_layers=2, mlp_hidden_size=200, decoder_layers=1, decoder_hidden_size=64, encoder_dropout=0, mlp_dropout=0, decoder_dropout=0, l2_reg=0.0, max_step_size=100, trade_off=0.8, epochs=200, batch_size=32, lr=0.001, optimizer='adam', grad_bound=5.0, transformer_encoder_layers=2, encoder_nhead=8, encoder_embedding_size=64, transformer_encoder_dropout=0.1, transformer_encoder_activation='relu', encoder_dim_feedforward=128, batch_first=True, d_latent_dim=64, transformer_decoder_layers=2, decoder_nhead=8, transformer_decoder_dropout=0.1, transformer_decoder_activation='relu', decoder_dim_feedforward=128, decoder_embedding_size=64, pre_train='True')\n",
      "INFO:  Construct Encoder with method transformerVae...\n",
      "INFO:  Construct Decoder with method transformerVae...\n",
      "INFO:  param size = 0.688371MB\n",
      "2\n",
      "Record: <__main__.SelectionRecord object at 0x000001CA71EFBF20>, Label: tensor([[0.9899],\n",
      "        [0.9899],\n",
      "        [0.9899],\n",
      "        [0.9899],\n",
      "        [0.9899],\n",
      "        [0.9899],\n",
      "        [0.9899],\n",
      "        [0.9899],\n",
      "        [0.9899],\n",
      "        [0.9899],\n",
      "        [0.9899],\n",
      "        [0.9899],\n",
      "        [0.9899],\n",
      "        [0.9899],\n",
      "        [0.9899],\n",
      "        [0.9899],\n",
      "        [0.9899],\n",
      "        [0.9899],\n",
      "        [0.9899],\n",
      "        [0.9899],\n",
      "        [0.9899],\n",
      "        [0.9899],\n",
      "        [0.9899],\n",
      "        [0.9899],\n",
      "        [0.9899],\n",
      "        [0.9899]])\n",
      "Record: <__main__.SelectionRecord object at 0x000001CA71EFAC00>, Label: tensor([[0.9829],\n",
      "        [0.9829],\n",
      "        [0.9829],\n",
      "        [0.9829],\n",
      "        [0.9829],\n",
      "        [0.9829],\n",
      "        [0.9829],\n",
      "        [0.9829],\n",
      "        [0.9829],\n",
      "        [0.9829],\n",
      "        [0.9829],\n",
      "        [0.9829],\n",
      "        [0.9829],\n",
      "        [0.9829],\n",
      "        [0.9829],\n",
      "        [0.9829],\n",
      "        [0.9829],\n",
      "        [0.9829],\n",
      "        [0.9829],\n",
      "        [0.9829],\n",
      "        [0.9829],\n",
      "        [0.9829],\n",
      "        [0.9829],\n",
      "        [0.9829],\n",
      "        [0.9829],\n",
      "        [0.9829]])\n",
      "2\n",
      "Record: <__main__.SelectionRecord object at 0x000001CA71EFBF20>, Label: tensor([[0.9899]])\n",
      "Record: <__main__.SelectionRecord object at 0x000001CA71EFAC00>, Label: tensor([[0.9829]])\n",
      "INFO:  Training Encoder-Predictor-Decoder\n",
      "INFO:  epoch 0001 train loss 1.716814 mse 0.252352 ce 7.574662, kl 15.101521\n",
      "[0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]\n",
      "[1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "INFO:  Evaluation on train data\n",
      "INFO:  epoch 0001 mse 0.245722 ce 6.910582 pairwise accuracy 0.892451 hamming distance 1851.884615\n",
      "[1.0, 0.0]\n",
      "INFO:  Evaluation on valid data\n",
      "INFO:  epoch 0001 mse 0.245714 ce 6.915237 pairwise accuracy 1.000000 hamming distance 1852.500000\n",
      "INFO:  epoch 0010 train loss 1.108052 mse 0.128127 ce 5.027752, kl 30.851784\n"
     ]
    }
   ],
   "source": [
    "baseline_name = [\n",
    "    'kbest',\n",
    "    'mrmr',\n",
    "    'lasso',\n",
    "    'rfe',\n",
    "    # 'gfs',\n",
    "    'lassonet',\n",
    "    'sarlfs',\n",
    "    'marlfs',\n",
    "\n",
    "]\n",
    "\n",
    "def gafs_train(train_queue, model: GAFS, optimizer):\n",
    "    objs = AvgrageMeter()\n",
    "    mse = AvgrageMeter()\n",
    "    nll = AvgrageMeter()\n",
    "    kl = AvgrageMeter()\n",
    "    model.train()\n",
    "    for step, sample in enumerate(train_queue):\n",
    "        encoder_input = sample['encoder_input']\n",
    "        encoder_target = sample['encoder_target']\n",
    "        decoder_input = sample['decoder_input']\n",
    "        decoder_target = sample['decoder_target']\n",
    "\n",
    "        # Excluding cuda\n",
    "\n",
    "        # encoder_input = encoder_input.cuda(model.gpu)\n",
    "        # encoder_target = encoder_target.cuda(model.gpu).requires_grad_()\n",
    "        # decoder_input = decoder_input.cuda(model.gpu)\n",
    "        # decoder_target = decoder_target.cuda(model.gpu)\n",
    "\n",
    "        encoder_input = encoder_input.to('cpu')\n",
    "        encoder_target = encoder_target.to('cpu').requires_grad_()\n",
    "        decoder_input = decoder_input.to('cpu')\n",
    "        decoder_target = decoder_target.to('cpu')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predict_value, log_prob, arch, mu, logvar= model.forward(encoder_input, decoder_input)\n",
    "        # 都没有除以batch size，如果需要除以batch size，令 reduction = \"mean\"\n",
    "        loss_1 = F.mse_loss(predict_value.squeeze(), encoder_target.squeeze()) # mse loss\n",
    "        loss_2 = F.nll_loss(log_prob.contiguous().view(-1, log_prob.size(-1)), decoder_target.view(-1)) # ce loss\n",
    "        if args.method_name == \"transformerVae\": \n",
    "            kl_loss = (-0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()))\n",
    "            if args.pre_train == 'True':\n",
    "                loss = args.trade_off * loss_1 + (1 - args.trade_off) * loss_2\n",
    "            else:\n",
    "                loss = loss = args.trade_off * loss_1 + (1 - args.trade_off) * loss_2 + 0.001 * kl_loss\n",
    "        elif args.method_name == \"transformer\": \n",
    "            kl_loss = torch.tensor(1, dtype=torch.long)\n",
    "            loss = args.trade_off * loss_1 + (1 - args.trade_off) * loss_2\n",
    "            # loss = loss_1 + loss_2\n",
    "            # l2_regularization = torch.tensor(0.0).cuda(model.gpu)\n",
    "            # for param in model.parameters():\n",
    "            #     l2_regularization += torch.norm(param, 2).cuda(model.gpu)\n",
    "            # loss += args.l2_reg * l2_regularization \n",
    "        else:\n",
    "            kl_loss = torch.tensor(1, dtype=torch.long)\n",
    "            loss = args.trade_off * loss_1 + (1 - args.trade_off) * loss_2\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_bound)\n",
    "        optimizer.step()\n",
    "\n",
    "        n = encoder_input.size(0)\n",
    "        objs.update(loss.data, n)\n",
    "        mse.update(loss_1.data, n)\n",
    "        nll.update(loss_2.data, n)\n",
    "        kl.update(kl_loss.data, n)\n",
    "    return objs.avg, mse.avg, nll.avg, kl.avg\n",
    "\n",
    "\n",
    "def gafs_valid(queue, model: GAFS):\n",
    "    pa = AvgrageMeter()\n",
    "    hs = AvgrageMeter()\n",
    "    mse = AvgrageMeter()\n",
    "    ce = AvgrageMeter()\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for step, sample in enumerate(queue):\n",
    "            encoder_input = sample['encoder_input']\n",
    "            encoder_target = sample['encoder_target']\n",
    "            decoder_input = sample['decoder_input']\n",
    "            decoder_target = sample['decoder_target']\n",
    "\n",
    "            # Excluding cuda\n",
    "\n",
    "            # encoder_input = encoder_input.cuda(model.gpu)\n",
    "            # encoder_target = encoder_target.cuda(model.gpu)\n",
    "            # decoder_input = decoder_input.cuda(model.gpu)\n",
    "            # decoder_target = decoder_target.cuda(model.gpu)\n",
    "\n",
    "            encoder_input = encoder_input.to('cpu')\n",
    "            encoder_target = encoder_target.to('cpu')\n",
    "            decoder_input = decoder_input.to('cpu')\n",
    "            decoder_target = decoder_target.to('cpu')\n",
    "\n",
    "            predict_value, logits, arch, mu, logvar = model.forward(encoder_input, decoder_input)\n",
    "            n = encoder_input.size(0)\n",
    "            # print(encoder_target)\n",
    "            pairwise_acc = pairwise_accuracy(encoder_target.data.squeeze().tolist(),\n",
    "                                             predict_value.data.squeeze().tolist())\n",
    "            hamming_dis = hamming_distance(decoder_target.data.squeeze().tolist(), arch.data.squeeze().tolist())\n",
    "            mse.update(F.mse_loss(predict_value.data.squeeze(), encoder_target.data.squeeze()), n)\n",
    "            pa.update(pairwise_acc, n)\n",
    "            hs.update(hamming_dis, n)\n",
    "            ce.update(F.nll_loss(logits.contiguous().view(-1, logits.size(-1)), decoder_target.view(-1)), n)\n",
    "    return mse.avg, pa.avg, hs.avg, ce.avg\n",
    "\n",
    "\n",
    "def choice_to_onehot(choice: List[int]):\n",
    "    size = len(choice)\n",
    "    onehot = torch.zeros(size + 1)\n",
    "    onehot[torch.tensor(choice)] = 1\n",
    "    return onehot[:-1]\n",
    "    # if choice.dim() == 1:\n",
    "    #     selected = torch.zeros_like(choice)\n",
    "    #     selected[choice] = 1\n",
    "    #     return selected[1:-1]\n",
    "    # else:\n",
    "    #     onehot = torch.empty_like(choice)\n",
    "    #     for i in range(choice.shape[0]):\n",
    "    #         onehot[i] = choice_to_onehot(choice[i])\n",
    "    #     return onehot\n",
    "\n",
    "\n",
    "def gafs_infer(queue, model, step, direction='+'):\n",
    "    new_gen_list = []\n",
    "    model.eval()\n",
    "    for i, sample in enumerate(queue):\n",
    "        encoder_input = sample['encoder_input']\n",
    "        encoder_input = encoder_input.to('cpu')\n",
    "        # Excluding cuda\n",
    "        # encoder_input = encoder_input.cuda(model.gpu)\n",
    "        model.zero_grad()\n",
    "        new_gen = model.generate_new_feature(encoder_input, predict_lambda=step, direction=direction)\n",
    "        new_gen_list.extend(new_gen.data.squeeze().tolist())\n",
    "    return new_gen_list\n",
    "\n",
    "\n",
    "def select_top_k(choice: Tensor, labels: Tensor, k: int) -> (Tensor, Tensor):\n",
    "    values, indices = torch.topk(labels, min(k, labels.shape[0]), dim=0)\n",
    "    return choice[indices.squeeze()], labels[indices.squeeze()]\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not torch.cuda.is_available():\n",
    "        info('No GPU found!')\n",
    "        #sys.exit(1)\n",
    "    # os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(str(x) for x in args.gpu)\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    # torch.cuda.manual_seed(args.seed)\n",
    "    # torch.cuda.manual_seed_all(args.seed)\n",
    "    # cudnn.enabled = True\n",
    "    # cudnn.benchmark = False\n",
    "    # cudnn.deterministic = True\n",
    "    # device = int(args.gpu)\n",
    "\n",
    "    device = 'cpu'\n",
    "    info(f\"Args = {args}\")\n",
    "\n",
    "    with open(f'{base_path}/history/{args.task_name}/fe.pkl', 'rb') as f:\n",
    "        fe: FeatureEvaluator = pickle.load(f)\n",
    "    model = GAFS(fe, args)\n",
    "    if args.pre_train == \"False\":\n",
    "        model.load_state_dict(torch.load(f'{base_path}/history/{args.task_name}/GAFS_pretrain_{args.method_name}.model_dict'))\n",
    "    elif args.pre_train == \"Search\":\n",
    "        model.load_state_dict(torch.load(f'{base_path}/history/{args.task_name}/GAFS_{args.method_name}.model_dict'))\n",
    "    \n",
    "    info(f\"param size = {count_parameters_in_MB(model)}MB\")\n",
    "    # Exculding cuda\n",
    "    # model = model.cuda(device)\n",
    "\n",
    "    # 设置为0表示不进行数据增强\n",
    "    choice, labels = fe.get_record(args.gen_num, eos=fe.ds_size)\n",
    "    valid_choice, valid_labels = fe.get_record(0, eos=fe.ds_size)\n",
    "\n",
    "    info('Training Encoder-Predictor-Decoder')\n",
    "    min_val = min(labels)\n",
    "    max_val = max(labels)\n",
    "    train_encoder_target = [(i - min_val) / (max_val - min_val) for i in labels] # min == max \n",
    "    valid_encoder_target = [(i - min_val) / (max_val - min_val) for i in valid_labels]\n",
    "\n",
    "    train_dataset = FSDataset(choice, train_encoder_target, train=True, sos_id=fe.ds_size, eos_id=fe.ds_size)\n",
    "    valid_dataset = FSDataset(valid_choice, valid_encoder_target, train=True, sos_id=fe.ds_size, eos_id=fe.ds_size)\n",
    "    train_queue = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=args.batch_size, shuffle=True, pin_memory=True)\n",
    "    valid_queue = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=len(valid_dataset), shuffle=False, pin_memory=True)\n",
    "    nao_optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.l2_reg)\n",
    "    save_model = model\n",
    "    cur_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    for nao_epoch in range(1, args.epochs + 1):\n",
    "        sys.stdout.flush()\n",
    "        sys.stderr.flush()\n",
    "        nao_loss, nao_mse, nao_ce, kl = gafs_train(train_queue, model, nao_optimizer)\n",
    "        if nao_epoch % 10 == 0 or nao_epoch == 1:\n",
    "            info(\"epoch {:04d} train loss {:.6f} mse {:.6f} ce {:.6f}, kl {:.6f}\".format(nao_epoch, nao_loss, nao_mse, nao_ce, kl))\n",
    "            if nao_loss < cur_loss:\n",
    "                save_model = model\n",
    "                cur_loss = nao_loss\n",
    "                best_epoch = nao_epoch\n",
    "        if nao_epoch % 100 == 0 or nao_epoch == 1:\n",
    "            mse, pa, hs, ce = gafs_valid(train_queue, model)\n",
    "            info(\"Evaluation on train data\")\n",
    "            info('epoch {:04d} mse {:.6f} ce {:.6f} pairwise accuracy {:.6f} hamming distance {:.6f}'.format(nao_epoch, mse, ce, pa,\n",
    "                                                                                                   hs))\n",
    "            mse, pa, hs, ce = gafs_valid(valid_queue, model)\n",
    "            info(\"Evaluation on valid data\")\n",
    "            info('epoch {:04d} mse {:.6f} ce {:.6f} pairwise accuracy {:.6f} hamming distance {:.6f}'.format(nao_epoch, mse, ce, pa,\n",
    "                                                                                                   hs))\n",
    "    model = save_model\n",
    "    info(\"best model from epoch {:04d}\".format(best_epoch))\n",
    "\n",
    "    top_selection, top_performance = select_top_k(valid_choice, valid_labels, args.top_k)\n",
    "\n",
    "    infer_dataset = FSDataset(top_selection, top_performance, False, sos_id=fe.ds_size, eos_id=fe.ds_size)\n",
    "    infer_queue = torch.utils.data.DataLoader(infer_dataset, batch_size=len(infer_dataset), shuffle=False,\n",
    "                             pin_memory=True)\n",
    "    if args.method_name != \"transformerVae\" or (args.method_name == \"transformerVae\" and args.pre_train != \"True\"):\n",
    "        new_selection = []\n",
    "        new_choice = []\n",
    "        predict_step_size = 0\n",
    "        while len(new_selection) < args.new_gen:\n",
    "            predict_step_size += 1\n",
    "            info('Generate new architectures with step size {:.2f}'.format(predict_step_size))\n",
    "            new_record = gafs_infer(infer_queue, model, direction='+', step=predict_step_size)\n",
    "            for choice in new_record:\n",
    "                onehot_choice = choice_to_onehot(choice)\n",
    "                if onehot_choice.sum() <= 0:\n",
    "                    error('insufficient selection')\n",
    "                    continue\n",
    "                record = SelectionRecord(onehot_choice.numpy(), -1)\n",
    "                if record not in fe.records.r_list and record not in new_selection:\n",
    "                    new_selection.append(record)\n",
    "                    new_choice.append(onehot_choice)\n",
    "                if len(new_selection) >= args.new_gen:\n",
    "                    break\n",
    "            info(f'{len(new_selection)} new choice generated now', )\n",
    "            if predict_step_size > args.max_step_size:\n",
    "                break\n",
    "        info(f'build {len(new_selection)} new choice !!!')\n",
    "\n",
    "        new_choice_pt = torch.stack(new_choice)\n",
    "        if args.gen_num == 0:\n",
    "            choice_path = f'{base_path}/history/{fe.task_name}/generated_choice_{args.method_name}.pt'\n",
    "        else:\n",
    "            choice_path = f'{base_path}/history/{fe.task_name}/generated_choice_{args.method_name}.pt'\n",
    "        torch.save(new_choice_pt, choice_path)\n",
    "        info(f'save generated choice to {choice_path}')\n",
    "\n",
    "    # previous_optimal = float(torch.max(valid_labels))\n",
    "    optimal_selection = None\n",
    "    if args.pre_train == \"True\":\n",
    "        torch.save(model.state_dict(), f'{base_path}/history/{fe.task_name}/GAFS_pretrain_{args.method_name}.model_dict')\n",
    "        torch.save(model.state_dict(), f'{base_path}/history/{fe.task_name}/GAFS_{args.method_name}.model_dict')\n",
    "    else:\n",
    "        torch.save(model.state_dict(), f'{base_path}/history/{fe.task_name}/GAFS_{args.method_name}.model_dict')\n",
    "    if args.pre_train == \"True\":\n",
    "        return -1\n",
    "    best_selection = None\n",
    "    best_optimal = -1000\n",
    "    best_selection_test = None\n",
    "    best_optimal_test = -1000\n",
    "    # info(f'the best performance for this task is {previous_optimal}')\n",
    "   \n",
    "    for s in new_selection:\n",
    "        train_data = fe.generate_data(s.operation, 'train')\n",
    "        result = fe.get_performance(train_data)\n",
    "        test_data = fe.generate_data(s.operation, 'test')\n",
    "        test_result = fe.get_performance(test_data)\n",
    "        # if result > previous_optimal:\n",
    "        #     optimal_selection = s.operation\n",
    "        #     previous_optimal = result\n",
    "        #     info(f'found optimal selection! the choice is {s.operation}, the performance on train is {result}')\n",
    "        if result > best_optimal:\n",
    "            best_selection = s.operation\n",
    "            best_optimal = result\n",
    "            info(f'found best on train : {best_optimal}')\n",
    "        if test_result > best_optimal_test:\n",
    "            best_selection_test = s.operation\n",
    "            best_optimal_test = test_result\n",
    "            info(f'found best on test : {best_optimal_test}')\n",
    "\n",
    "    opt_path = f'{base_path}/history/{fe.task_name}/best-ours.hdf'\n",
    "    ori_p = fe.report_performance(best_selection, flag='test')\n",
    "    info(f'found train generation in our method! the choice is {best_selection}, the performance is {ori_p}')\n",
    "    fe.generate_data(best_selection, 'train').to_hdf(opt_path, key='train')\n",
    "    fe.generate_data(best_selection, 'test').to_hdf(opt_path, key='test')\n",
    "\n",
    "    opt_path_test = f'{base_path}/history/{fe.task_name}/best-ours-test.hdf'\n",
    "    test_p = fe.report_performance(best_selection_test, flag='test')\n",
    "    info(f'found test generation in our method! the choice is {best_selection_test}, the performance is {test_p}')\n",
    "    fe.generate_data(best_selection_test, 'train').to_hdf(opt_path_test, key='train')\n",
    "    fe.generate_data(best_selection_test, 'test').to_hdf(opt_path_test, key='test')\n",
    "    ps = []\n",
    "    info('given overall validation')\n",
    "    # Excluding baseline reports\n",
    "    # report_head = 'RAW\\t'\n",
    "    # raw_test = pandas.read_hdf(f'{base_path}/history/{fe.task_name}.hdf', key='raw_test')\n",
    "    # ps.append('{:.2f}'.format(fe.get_performance(raw_test) * 100))\n",
    "    # for method in baseline_name:\n",
    "    #     report_head += f'{method}\\t'\n",
    "    #     spe_test = pandas.read_hdf(f'{base_path}/history/{fe.task_name}.hdf', key=f'{method}_test')\n",
    "    #     ps.append('{:.2f}'.format(fe.get_performance(spe_test) * 100))\n",
    "    # report_head += 'Ours\\tOurs_Test'\n",
    "    # print(report_head)\n",
    "    report = ''\n",
    "    for per in ps:\n",
    "        report += f'{per}&\\t'\n",
    "    report += '{:.2f}&\\t'.format(ori_p * 100)\n",
    "    report += '{:.2f}&\\t'.format(test_p * 100)\n",
    "    print(report)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
