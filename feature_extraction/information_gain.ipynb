{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Information Gain Results\n",
    "\n",
    "## What is Information Gain?\n",
    "Information Gain measures the reduction in uncertainty about a target variable provided by knowing the value of a feature. It is widely used in decision trees and feature selection to identify the most informative features.\n",
    "\n",
    "- [Information Gain and Mutual Information for Machine Learning](https://machinelearningmastery.com/information-gain-and-mutual-information/)\n",
    "- [Feature Selection Techniques in Machine Learning](https://www.geeksforgeeks.org/feature-selection-techniques-in-machine-learning/#:~:text=techniques%20used%20are%3A-,Information%20Gain,-%E2%80%93%20It%20is)\n",
    "\n",
    "### How to Interpret the Results\n",
    "- **High Information Gain**: Indicates that the feature strongly relates to the target variable. Features with higher values should be prioritized for predictive modeling.\n",
    "- **Low Information Gain**: Suggests that the feature has little to no predictive power for the target variable.\n",
    "\n",
    "### Practical Uses of Information Gain\n",
    "1. **Feature Selection**: \n",
    "   - Retain features with high Information Gain to reduce dimensionality and improve model performance.\n",
    "   - Discard features with very low Information Gain, as they contribute minimal predictive power.\n",
    "\n",
    "2. **Feature Importance Analysis**: \n",
    "   - Understand which features are most relevant for the target variables (`stage` and `subtype`).\n",
    "   - Guide domain experts to focus on significant variables for further analysis.\n",
    "\n",
    "3. **Improving Model Efficiency**: \n",
    "   - By focusing on the top features, reduce the computational burden for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# === CONFIGURABLE PARAMETERS ===\n",
    "SCALE_DATA = True\n",
    "RANDOM_STATE = 42\n",
    "PREPROCESSED_FILE = \"preprocessed_miRNA_data.csv\"\n",
    "# Hardcoding random_state might ensure consistent results\n",
    "# Remove if needing more variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data for the first time...\n",
      "Preprocessed data saved at: preprocessed_miRNA_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the uploaded dataset\n",
    "if os.path.exists(PREPROCESSED_FILE):\n",
    "    print(\"Loading preprocessed data...\")\n",
    "    data_scaled = pd.read_csv(PREPROCESSED_FILE)\n",
    "    \n",
    "    # Extract features and targets\n",
    "    X = data_scaled.drop(columns=['stage', 'subtype', 'general'])\n",
    "    y_general = data_scaled['general']\n",
    "    y_stage_encoded = data_scaled['stage']\n",
    "    y_subtype_encoded = data_scaled['subtype']\n",
    "    \n",
    "else:\n",
    "    print(\"Preprocessing data for the first time...\")\n",
    "    \n",
    "    # Load raw data\n",
    "    file_path = \"miRNA_stage_subtype.csv\"\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Create 'General' Classification\n",
    "    data['general'] = (data['stage'] > 0).astype(int)\n",
    "\n",
    "    # Separate features and targets\n",
    "    X = data.drop(columns=['stage', 'subtype', 'general'])\n",
    "    y_general = data['general']\n",
    "    y_stage = data['stage']\n",
    "    y_subtype = data['subtype']\n",
    "\n",
    "    # Scale Features (if enabled)\n",
    "    if SCALE_DATA:\n",
    "        scaler = StandardScaler()\n",
    "        X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "    # Encode categorical targets if necessary\n",
    "    le_stage = LabelEncoder()\n",
    "    y_stage_encoded = le_stage.fit_transform(y_stage)\n",
    "\n",
    "    le_subtype = LabelEncoder()\n",
    "    y_subtype_encoded = le_subtype.fit_transform(y_subtype)\n",
    "\n",
    "    # Save preprocessed data\n",
    "    data_scaled = X.copy()\n",
    "    data_scaled['general'] = y_general\n",
    "    data_scaled['stage'] = y_stage_encoded\n",
    "    data_scaled['subtype'] = y_subtype_encoded\n",
    "    data_scaled.to_csv(PREPROCESSED_FILE, index=False)\n",
    "    print(f\"Preprocessed data saved at: {PREPROCESSED_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Features for 'General' Classification:\n",
      "             Feature  Info_Gain_General\n",
      "1013    hsa-mir-4731           0.010480\n",
      "541     hsa-mir-3661           0.009895\n",
      "680     hsa-mir-4257           0.009355\n",
      "1341    hsa-mir-5690           0.009244\n",
      "225     hsa-mir-181d           0.009049\n",
      "914     hsa-mir-4637           0.008743\n",
      "1208  hsa-mir-526a-2           0.008537\n",
      "1312  hsa-mir-5583-1           0.008408\n",
      "274     hsa-mir-203b           0.008402\n",
      "1136   hsa-mir-509-1           0.008129\n",
      "\n",
      "Top 10 Features for 'Stage' Classification:\n",
      "           Feature  Info_Gain_Stage\n",
      "1680  hsa-mir-6858         0.052485\n",
      "842   hsa-mir-4490         0.049394\n",
      "1340   hsa-mir-569         0.048567\n",
      "1628  hsa-mir-6806         0.047535\n",
      "1417   hsa-mir-608         0.046619\n",
      "1767  hsa-mir-7704         0.046536\n",
      "1133  hsa-mir-5087         0.045498\n",
      "1125   hsa-mir-502         0.045280\n",
      "1099   hsa-mir-490         0.044939\n",
      "1769  hsa-mir-7706         0.044834\n",
      "\n",
      "Top 10 Features for 'Subtype' Classification:\n",
      "             Feature  Info_Gain_Subtype\n",
      "1496     hsa-mir-651           0.052176\n",
      "1040    hsa-mir-4758           0.048166\n",
      "779     hsa-mir-4434           0.046856\n",
      "16    hsa-mir-103a-1           0.046329\n",
      "1724     hsa-mir-7-3           0.045904\n",
      "724     hsa-mir-4299           0.045544\n",
      "159   hsa-mir-1302-8           0.044803\n",
      "852     hsa-mir-449a           0.044419\n",
      "1586    hsa-mir-6768           0.040701\n",
      "441   hsa-mir-3180-3           0.040596\n"
     ]
    }
   ],
   "source": [
    "# === STEP 2: Compute Information Gain ===\n",
    "info_gain_general = mutual_info_classif(X, y_general, random_state=RANDOM_STATE)\n",
    "info_gain_stage = mutual_info_classif(X, y_stage_encoded, random_state=RANDOM_STATE)\n",
    "info_gain_subtype = mutual_info_classif(X, y_subtype_encoded, random_state=RANDOM_STATE)\n",
    "\n",
    "# Combine results into a DataFrame\n",
    "info_gain_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Info_Gain_General': info_gain_general,\n",
    "    'Info_Gain_Stage': info_gain_stage,\n",
    "    'Info_Gain_Subtype': info_gain_subtype\n",
    "})\n",
    "\n",
    "info_gain_df_sorted = info_gain_df.sort_values(by=['Info_Gain_General', 'Info_Gain_Stage', 'Info_Gain_Subtype'], ascending=False)\n",
    "\n",
    "# Display the top 10 features for each classification level\n",
    "top_features_general = info_gain_df_sorted[['Feature', 'Info_Gain_General']].sort_values(\n",
    "    by='Info_Gain_General', ascending=False).head(10)\n",
    "\n",
    "top_features_stage = info_gain_df_sorted[['Feature', 'Info_Gain_Stage']].sort_values(\n",
    "    by='Info_Gain_Stage', ascending=False).head(10)\n",
    "\n",
    "top_features_subtype = info_gain_df_sorted[['Feature', 'Info_Gain_Subtype']].sort_values(\n",
    "    by='Info_Gain_Subtype', ascending=False).head(10)\n",
    "\n",
    "print(\"Top 10 Features for 'General' Classification:\")\n",
    "print(top_features_general)\n",
    "\n",
    "print(\"\\nTop 10 Features for 'Stage' Classification:\")\n",
    "print(top_features_stage)\n",
    "\n",
    "print(\"\\nTop 10 Features for 'Subtype' Classification:\")\n",
    "print(top_features_subtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
